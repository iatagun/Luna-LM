# Luna-LM Foundation Model EÄŸitim Rehberi

TÃ¼rkÃ§e foundation language model (Luna-LM) eÄŸitimi iÃ§in tam rehber.

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Gereksinimleri YÃ¼kle

```bash
pip install torch transformers tokenizers matplotlib tqdm
```

### 2. Model EÄŸitimi BaÅŸlat

```bash
python train_luna_lm.py
```

**Ä°lk eÄŸitim ~2-4 saat sÃ¼rebilir** (GPU'ya baÄŸlÄ±)

### 3. EÄŸitilmiÅŸ Modeli Test Et

```bash
python inference_luna_lm.py
```

---

## ğŸ“Š Model BoyutlarÄ±

EÄŸitim scripti (`train_luna_lm.py`) iÃ§inde `MODEL_SIZE` deÄŸiÅŸkenini deÄŸiÅŸtirerek model boyutunu seÃ§ebilirsiniz:

| Model Size | Parametreler | RAM Gereksinimi | EÄŸitim SÃ¼resi* | Ã–nerilen KullanÄ±m |
|------------|--------------|-----------------|----------------|-------------------|
| **tiny**   | ~10M         | 2-4 GB          | 30-60 dk       | HÄ±zlÄ± test        |
| **small**  | ~50M         | 4-8 GB          | 2-4 saat       | âœ… **Ã–nerilen**   |
| **medium** | ~150M        | 8-16 GB         | 6-12 saat      | Ä°yi sonuÃ§lar      |

\* GTX 1660 Ti / RTX 2060 seviyesi GPU iÃ§in yaklaÅŸÄ±k sÃ¼reler

---

## ğŸ“ Dosya YapÄ±sÄ±

```
Luna-LM/
â”œâ”€â”€ foundation_corpus.txt              # EÄŸitim verisi (25K+ satÄ±r)
â”‚
â”œâ”€â”€ train_luna_lm.py                   # â­ ANA EÄÄ°TÄ°M SCRÄ°PTÄ°
â”œâ”€â”€ inference_luna_lm.py               # Model test ve kullanÄ±m
â”‚
â”œâ”€â”€ turkish_tokenizer_pretrained.py    # Tokenizer (pretrained)
â”œâ”€â”€ turkish_tokenizer_huggingface.py   # Tokenizer (HF custom)
â”œâ”€â”€ turkish_tokenizer_training.py      # Tokenizer (sÄ±fÄ±rdan)
â”‚
â””â”€â”€ luna_lm_checkpoints_YYYYMMDD_HHMMSS/  # EÄŸitim Ã§Ä±ktÄ±larÄ±
    â”œâ”€â”€ best_model.pt                  # En iyi model checkpoint
    â”œâ”€â”€ epoch_1.pt, epoch_2.pt, ...    # Her epoch'un checkpointi
    â”œâ”€â”€ config.json                     # Model konfigÃ¼rasyonu
    â””â”€â”€ training_loss.png               # Loss grafiÄŸi
```

---

## ğŸ¯ AdÄ±m AdÄ±m Rehber

### AdÄ±m 1: Model Boyutu SeÃ§

`train_luna_lm.py` dosyasÄ±nÄ± aÃ§Ä±n ve 277. satÄ±rÄ± dÃ¼zenleyin:

```python
MODEL_SIZE = "small"  # "tiny", "small", veya "medium"
```

**Ã–nerilen baÅŸlangÄ±Ã§:** `"small"` (50M parametre)

---

### AdÄ±m 2: Hyperparameter AyarlarÄ± (Opsiyonel)

`train_luna_lm.py` iÃ§inde 297-302. satÄ±rlarda:

```python
BATCH_SIZE = 4           # GPU memory'e gÃ¶re ayarlayÄ±n
CONTEXT_LENGTH = 256     # Daha uzun context = daha fazla memory
NUM_EPOCHS = 10          # EÄŸitim epoch sayÄ±sÄ±
LEARNING_RATE = 3e-4     # Standart GPT learning rate
EVAL_FREQ = 100          # Her 100 step'te bir deÄŸerlendirme
EVAL_ITER = 10           # DeÄŸerlendirme iÃ§in batch sayÄ±sÄ±
```

**Memory problemi varsa:**
- `BATCH_SIZE` dÃ¼ÅŸÃ¼rÃ¼n (2 veya 1)
- `CONTEXT_LENGTH` dÃ¼ÅŸÃ¼rÃ¼n (128)
- `MODEL_SIZE = "tiny"` seÃ§in

---

### AdÄ±m 3: EÄŸitimi BaÅŸlat

```bash
python train_luna_lm.py
```

**EÄŸitim sÄ±rasÄ±nda gÃ¶recekleriniz:**

```
==============================================================
LUNA-LM FOUNDATION MODEL EÄÄ°TÄ°MÄ°
==============================================================

1. Hyperparameter konfigÃ¼rasyonu...
  Device: cuda
  Model size: small
  Batch size: 4
  Context length: 256
  Epochs: 10
  Learning rate: 0.0003

2. Tokenizer yÃ¼kleniyor...
  âœ“ Vocab size: 32,000

3. Corpus yÃ¼kleniyor...
  âœ“ Corpus boyutu: 2,583,200 karakter
  âœ“ Train: 2,324,880 karakter
  âœ“ Val: 258,320 karakter

4. DataLoader oluÅŸturuluyor...
  âœ“ Train batches: 5,680
  âœ“ Val batches: 632

5. Model oluÅŸturuluyor...
  âœ“ Model hazÄ±r!
    Toplam parametreler: 52,428,800 (52.4M)
    Layers: 6
    Heads: 8
    Embedding dim: 512

==============================================================
EÄÄ°TÄ°M BAÅLIYOR
==============================================================

Epoch 1/10 | Step 100 | Train Loss: 8.2456 | Val Loss: 8.1234
  âœ“ En iyi model kaydedildi! Val Loss: 8.1234

  ğŸ“ Ã–rnek metin Ã¼retimi:
  'BugÃ¼n hava Ã§ok gÃ¼zel, gÃ¼neÅŸ parlÄ±yor ve insanlar dÄ±ÅŸarÄ±da...'

Epoch 1/10 | Step 200 | Train Loss: 7.8923 | Val Loss: 7.7654
...
```

**EÄŸitim sÃ¼resi:**
- **tiny** model: ~30-60 dakika
- **small** model: ~2-4 saat
- **medium** model: ~6-12 saat

---

### AdÄ±m 4: EÄŸitimi Ä°zleme

#### A. Terminalde Real-time
- Loss deÄŸerleri her 100 step'te yazdÄ±rÄ±lÄ±r
- Ã–rnek metin Ã¼retimleri her 500 step'te gÃ¶sterilir

#### B. Loss GrafiÄŸi
EÄŸitim bitince `training_loss.png` oluÅŸur:
- Train loss (mavi)
- Validation loss (turuncu)
- Loss dÃ¼ÅŸÃ¼yorsa âœ… iyi gidiyor
- Val loss artÄ±yorsa âš ï¸ overfitting

---

### AdÄ±m 5: Modeli Test Et

EÄŸitim bitince:

```bash
python inference_luna_lm.py
```

**Test Ã§Ä±ktÄ±sÄ±:**

```
==============================================================
LUNA-LM INFERENCE
==============================================================

Device: cuda

Checkpoint dizini: luna_lm_checkpoints_20251214_153045

Model yÃ¼kleniyor...
  Config yÃ¼klendi:
    Vocab size: 32,000
    Layers: 6
    Embedding dim: 512
  âœ“ Model yÃ¼klendi: best_model.pt
    Epoch: 9
    Global step: 56,800
    Val loss: 4.2345

==============================================================
TEST ÃœRETÄ°MLERÄ°
==============================================================

ğŸ“ Prompt: 'BugÃ¼n hava Ã§ok gÃ¼zel'
ğŸ¤– Luna-LM:
BugÃ¼n hava Ã§ok gÃ¼zel, gÃ¶kyÃ¼zÃ¼ aÃ§Ä±k ve gÃ¼neÅŸ parlÄ±yor. 
Ä°nsanlar parklarda yÃ¼rÃ¼yÃ¼ÅŸ yapÄ±yor, Ã§ocuklar oyun 
oynuyor...

ğŸ“ Prompt: 'Yapay zekÃ¢ teknolojisi'
ğŸ¤– Luna-LM:
Yapay zekÃ¢ teknolojisi son yÄ±llarda bÃ¼yÃ¼k geliÅŸmeler 
gÃ¶sterdi. Makine Ã¶ÄŸrenimi algoritmalarÄ±...

Ä°nteraktif moda geÃ§mek ister misiniz? (y/n): y

==============================================================
LUNA-LM Ä°NTERAKTÄ°F MOD
==============================================================

ğŸ“ Prompt: Tarih boyunca insanlÄ±k
ğŸ¤– Luna-LM:
Tarih boyunca insanlÄ±k birÃ§ok zorlukla karÅŸÄ±laÅŸtÄ±...

ğŸ“ Prompt: quit
GÃ¶rÃ¼ÅŸmek Ã¼zere! ğŸ‘‹
```

---

## ğŸ›ï¸ Ä°nteraktif Mod Parametreleri

Ä°nteraktif modda `params` yazarak ayarlarÄ± deÄŸiÅŸtirebilirsiniz:

```
ğŸ“ Prompt: params

Mevcut parametreler:
  max_tokens: 100
  temperature: 0.8
  top_k: 50
  
Yeni max_tokens: 150
Yeni temperature: 1.2
Yeni top_k: 100
âœ“ Parametreler gÃ¼ncellendi!
```

**Parametre AÃ§Ä±klamalarÄ±:**

- **max_tokens**: Ãœretilecek maksimum kelime sayÄ±sÄ± (50-500 arasÄ±)
- **temperature**: YaratÄ±cÄ±lÄ±k seviyesi
  - `0.1-0.5`: Deterministik, tutarlÄ±
  - `0.7-1.0`: Dengeli âœ… **Ã¶nerilen**
  - `1.0-2.0`: YaratÄ±cÄ±, Ã§eÅŸitli
- **top_k**: Kelime havuzu bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (30-100 arasÄ±)

---

## ğŸ“ˆ EÄŸitim Ä°yileÅŸtirme

### Loss DÃ¼ÅŸmÃ¼yorsa:

1. **Learning rate'i artÄ±r:**
   ```python
   LEARNING_RATE = 5e-4  # 3e-4 yerine
   ```

2. **Daha fazla epoch:**
   ```python
   NUM_EPOCHS = 20  # 10 yerine
   ```

3. **Batch size artÄ±r** (GPU memory varsa):
   ```python
   BATCH_SIZE = 8  # 4 yerine
   ```

### Overfitting Varsa (Val loss artÄ±yor):

1. **Dropout artÄ±r** (`train_luna_lm.py`, model_config):
   ```python
   "drop_rate": 0.2,  # 0.1 yerine
   ```

2. **Weight decay artÄ±r:**
   ```python
   optimizer = torch.optim.AdamW(
       model.parameters(), 
       lr=LEARNING_RATE, 
       weight_decay=0.2  # 0.1 yerine
   )
   ```

3. **Daha fazla veri** ekleyin `foundation_corpus.txt`'e

---

## ğŸ”§ Memory Optimizasyonu

### GPU Memory Yetersiz HatasÄ±:

```
RuntimeError: CUDA out of memory
```

**Ã‡Ã¶zÃ¼mler:**

1. **Batch size dÃ¼ÅŸÃ¼r:**
   ```python
   BATCH_SIZE = 2  # veya 1
   ```

2. **Context length dÃ¼ÅŸÃ¼r:**
   ```python
   CONTEXT_LENGTH = 128  # 256 yerine
   ```

3. **KÃ¼Ã§Ã¼k model seÃ§:**
   ```python
   MODEL_SIZE = "tiny"
   ```

4. **Gradient accumulation** (geliÅŸmiÅŸ):
   ```python
   # train_luna_lm.py iÃ§inde, optimizer.step() Ã¶ncesi:
   if (batch_idx + 1) % 4 == 0:  # Her 4 batch'te bir
       optimizer.step()
       optimizer.zero_grad()
   ```

---

## ğŸ’¾ Checkpoint KullanÄ±mÄ±

### EÄŸitimi Devam Ettirme

EÄŸitim yarÄ±da kesildiyse:

```python
# train_luna_lm.py iÃ§inde, model oluÅŸturulduktan sonra:

checkpoint = torch.load('luna_lm_checkpoints_XXXXXXXX/epoch_5.pt')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch'] + 1
```

### En Ä°yi Modeli Kullanma

```python
# inference_luna_lm.py otomatik olarak best_model.pt kullanÄ±r
# Manuel yÃ¼kleme iÃ§in:

model, tokenizer, config = load_model(
    'luna_lm_checkpoints_XXXXXXXX',
    checkpoint_name='best_model.pt'  # veya 'epoch_10.pt'
)
```

---

## ğŸ“ Sonraki AdÄ±mlar

### 1. Fine-tuning (Ã–zel GÃ¶revler Ä°Ã§in)

Modelinizi belirli gÃ¶revler iÃ§in fine-tune edin:

- **Text Classification**: `LLMs-from-scratch/ch06/`
- **Instruction Following**: `LLMs-from-scratch/ch07/`

### 2. Daha Fazla Veri

`foundation_corpus.txt`'e daha fazla TÃ¼rkÃ§e metin ekleyin:
- Wikipedia makaleleri
- Kitaplar (telif hakkÄ± olmayan)
- Haberler
- Akademik metinler

### 3. Tokenizer Optimizasyonu

Custom tokenizer ile daha iyi sonuÃ§lar:

```bash
python turkish_tokenizer_huggingface.py  # Custom eÄŸit
```

Sonra `train_luna_lm.py` iÃ§inde tokenizer'Ä± deÄŸiÅŸtir:

```python
from turkish_tokenizer_huggingface import HFTokenizerWrapper
tokenizer = HFTokenizerWrapper('turkish_hf_tokenizer.json')
```

### 4. Daha BÃ¼yÃ¼k Model

GPU memory yetiyorsa:

```python
MODEL_SIZE = "medium"  # 150M parametre
```

veya custom config:

```python
model_config = {
    "vocab_size": vocab_size,
    "context_length": 512,      # Daha uzun context
    "emb_dim": 1024,            # Daha bÃ¼yÃ¼k
    "n_heads": 16,
    "n_layers": 12,             # Daha derin
    "drop_rate": 0.1,
    "qkv_bias": False
}
```

---

## ğŸ› SÄ±k Sorunlar

### 1. "No module named 'transformers'"
```bash
pip install transformers
```

### 2. "CUDA out of memory"
- Batch size dÃ¼ÅŸÃ¼r
- Model size kÃ¼Ã§Ã¼lt
- Context length azalt

### 3. "Loss NaN oluyor"
- Learning rate dÃ¼ÅŸÃ¼r: `LEARNING_RATE = 1e-4`
- Gradient clipping kontrol et (zaten var)

### 4. "EÄŸitim Ã§ok yavaÅŸ"
- GPU kullandÄ±ÄŸÄ±nÄ±zdan emin olun
- `torch.cuda.is_available()` True dÃ¶nmeli
- KÃ¼Ã§Ã¼k model ile test edin

### 5. "Ãœretilen metinler anlamsÄ±z"
- Daha fazla epoch eÄŸitin
- Loss 3.0'Ä±n altÄ±na dÃ¼ÅŸmeli
- Temperature ayarÄ±nÄ± deÄŸiÅŸtirin (0.7-1.0)

---

## ğŸ“Š Beklenen SonuÃ§lar

### Loss DeÄŸerleri

| Epoch | Train Loss | Val Loss | Metin Kalitesi |
|-------|-----------|----------|----------------|
| 1     | 8.5       | 8.3      | AnlamsÄ±z       |
| 3     | 6.2       | 6.1      | Hece/kelime    |
| 5     | 4.8       | 4.9      | Kelime dizileri|
| 10    | 3.2       | 3.5      | CÃ¼mleler âœ…    |
| 20    | 2.5       | 2.8      | MantÄ±klÄ± metinler ğŸ‰ |

**Not:** foundation_corpus.txt boyutuna gÃ¶re deÄŸiÅŸir

---

## ğŸ“š Ek Kaynaklar

- **LLMs from Scratch KitabÄ±**: [GitHub](https://github.com/rasbt/LLMs-from-scratch)
- **Transformer Paper**: "Attention is All You Need"
- **GPT-2 Paper**: "Language Models are Unsupervised Multitask Learners"

---

## âœ… Kontrol Listesi

EÄŸitime baÅŸlamadan Ã¶nce:

- [ ] `foundation_corpus.txt` dosyasÄ± var
- [ ] PyTorch kurulu (`torch.cuda.is_available()` kontrol)
- [ ] transformers paketi kurulu
- [ ] En az 4GB GPU memory (veya tiny model kullan)
- [ ] `train_luna_lm.py` dosyasÄ± hazÄ±r
- [ ] Model boyutu seÃ§ildi

EÄŸitim sonrasÄ±:

- [ ] `best_model.pt` oluÅŸtu
- [ ] Loss grafiÄŸi dÃ¼ÅŸÃ¼ÅŸ gÃ¶steriyor
- [ ] `inference_luna_lm.py` Ã§alÄ±ÅŸÄ±yor
- [ ] Ãœretilen metinler mantÄ±klÄ±

---

## ğŸ‰ BaÅŸarÄ±lar!

Luna-LM'inizi eÄŸitip kullanmaya baÅŸladÄ±ktan sonra:

1. Fine-tuning ile Ã¶zel gÃ¶revlere adapte edin
2. Daha fazla veri ile iyileÅŸtirin
3. Hyperparameter tuning yapÄ±n
4. Toplulukla paylaÅŸÄ±n!

**SorularÄ±nÄ±z iÃ§in:** GitHub Issues veya Discussions kullanabilirsiniz.
