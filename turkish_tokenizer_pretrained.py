"""
Mevcut TÃ¼rkÃ§e modellerden tokenizer kullanma
En hÄ±zlÄ± yÃ¶ntem - eÄŸitim gerektirmez
"""

from transformers import AutoTokenizer
import torch
from torch.utils.data import Dataset, DataLoader


class PretrainedTurkishTokenizer:
    """Ã–nceden eÄŸitilmiÅŸ TÃ¼rkÃ§e tokenizer wrapper"""
    
    def __init__(self, model_name='bert-base-turkish-cased'):
        """
        TÃ¼rkÃ§e destekli popÃ¼ler modeller:
        - 'bert-base-turkish-cased': DBMDz tarafÄ±ndan eÄŸitilmiÅŸ
        - 'dbmdz/bert-base-turkish-128k-cased': Daha bÃ¼yÃ¼k vocab
        - 'xlm-roberta-base': Ã‡ok dilli, TÃ¼rkÃ§e dahil
        - 'facebook/mbart-large-50': 50 dil destekli
        """
        print(f"Tokenizer yÃ¼kleniyor: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.vocab_size = len(self.tokenizer)
        
        # Ã–zel tokenlar ekle
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"âœ“ Tokenizer yÃ¼klendi: {self.vocab_size} token")
        print(f"  Pad token: {self.tokenizer.pad_token}")
        print(f"  EOS token: {self.tokenizer.eos_token}")
    
    def encode(self, text, allowed_special=None):
        """GPT kodu ile uyumlu encode"""
        return self.tokenizer.encode(text, add_special_tokens=False)
    
    def decode(self, token_ids):
        """Token ID'leri metne Ã§evir"""
        if isinstance(token_ids, torch.Tensor):
            token_ids = token_ids.tolist()
        return self.tokenizer.decode(token_ids, skip_special_tokens=False)
    
    def __getitem__(self, key):
        """Tokenizer'a dictionary-style eriÅŸim iÃ§in"""
        return self.tokenizer[key]
    
    def __len__(self):
        """Tokenizer vocab boyutu"""
        return len(self.tokenizer)


class GPTDatasetPretrained(Dataset):
    """Pretrained tokenizer kullanan GPT Dataset"""
    
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize - HuggingFace tokenizer'larÄ±n internal limitini bypass et
        # BÃ¼yÃ¼k text'leri chunk'lara bÃ¶lerek encode et
        chunk_size = 100000  # 100K karakter chunk'lar
        token_ids = []
        
        for i in range(0, len(txt), chunk_size):
            chunk = txt[i:i + chunk_size]
            chunk_tokens = tokenizer.tokenizer.encode(chunk, add_special_tokens=False)
            token_ids.extend(chunk_tokens)
        
        print(f"Toplam token: {len(token_ids)}")

        # Sliding window
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))
        
        print(f"OluÅŸturulan sample sayÄ±sÄ±: {len(self.input_ids)}")

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]


def create_dataloader_pretrained(txt, tokenizer, batch_size=4, max_length=256,
                                 stride=128, shuffle=True, drop_last=True):
    """Pretrained tokenizer ile DataLoader"""
    
    dataset = GPTDatasetPretrained(txt, tokenizer, max_length, stride)
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=shuffle, 
        drop_last=drop_last
    )
    return dataloader


# ==================== KULLANIM Ã–RNEÄžÄ° ====================
if __name__ == "__main__":
    print("=== Pretrained TÃ¼rkÃ§e Tokenizer Ã–rneÄŸi ===\n")
    
    # FarklÄ± tokenizer seÃ§eneklerini test et
    tokenizer_options = [
        'dbmdz/bert-base-turkish-cased',
        # 'xlm-roberta-base',  # Ã‡ok dilli
        # 'facebook/mbart-large-50',  # 50 dil
    ]
    
    for model_name in tokenizer_options:
        print(f"\n{'='*60}")
        print(f"Model: {model_name}")
        print('='*60)
        
        # 1. Tokenizer yÃ¼kle
        tokenizer = PretrainedTurkishTokenizer(model_name)
        
        # 2. Test cÃ¼mleleri
        print("\nðŸ“ Test Ã–rnekleri:")
        test_texts = [
            "Merhaba dÃ¼nya! BugÃ¼n hava Ã§ok gÃ¼zel.",
            "Yapay zekÃ¢ ve derin Ã¶ÄŸrenme Ã§alÄ±ÅŸmalarÄ±.",
            "TÃ¼rkÃ§e karakterler: ÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄžÃœÅžÄ°Ã–Ã‡"
        ]
        
        for text in test_texts:
            encoded = tokenizer.encode(text)
            decoded = tokenizer.decode(encoded)
            print(f"\n  Orijinal: {text}")
            print(f"  Token sayÄ±sÄ±: {len(encoded)}")
            print(f"  Encoded: {encoded[:10]}{'...' if len(encoded) > 10 else ''}")
            print(f"  Decoded: {decoded}")
        
        # 3. DataLoader test
        print("\nðŸ“Š DataLoader Test:")
        with open('foundation_corpus.txt', 'r', encoding='utf-8') as f:
            corpus = f.read()[:10000]  # Ä°lk 10K karakter
        
        dataloader = create_dataloader_pretrained(
            corpus,
            tokenizer,
            batch_size=2,
            max_length=128
        )
        
        print(f"âœ“ DataLoader hazÄ±r: {len(dataloader)} batch")
        
        # Ä°lk batch'i gÃ¶ster
        inputs, targets = next(iter(dataloader))
        print(f"\n  Batch shape:")
        print(f"    Input: {inputs.shape}")
        print(f"    Target: {targets.shape}")
        print(f"  Decoded (ilk 80 karakter):")
        print(f"    {tokenizer.decode(inputs[0])[:80]}...")
        
        break  # Ä°lk tokenizer yeterli


print("\n" + "="*60)
print("Ã–NERÄ°LER:")
print("="*60)
print("""
1. HIZLI BAÅžLANGIÃ‡ (Ã–nerilen):
   - dbmdz/bert-base-turkish-cased kullanÄ±n
   - EÄŸitim gerektirmez, hemen kullanÄ±ma hazÄ±r
   - 32K vocab size

2. DAHA BÃœYÃœK VOCAB:
   - dbmdz/bert-base-turkish-128k-cased (128K vocab)
   - Daha fazla kelime/subword kapsar

3. Ã‡OK DÄ°LLÄ° PROJE Ä°Ã‡Ä°N:
   - xlm-roberta-base (100+ dil)
   - TÃ¼rkÃ§e + diÄŸer dillerde Ã§alÄ±ÅŸacaksa

4. CUSTOM TOKENIZER (Ä°leri Seviye):
   - turkish_tokenizer_huggingface.py kullanÄ±n
   - Foundation corpus'unuza Ã¶zel optimize edilir
   - En iyi performans iÃ§in Ã¶nerilir
""")
