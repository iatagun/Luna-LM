"""
Luna-LM Model Inference
EÄŸitilmiÅŸ modeli yÃ¼kleyip metin Ã¼retimi yapar
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import glob

from luna.utils import load_model
from luna.generate import generate_text


# ==================== INFERENCE FONKSÄ°YONLARI ====================

def interactive_mode(model, tokenizer, device, model_config):
    """Ä°nteraktif mod â€” kullanÄ±cÄ±dan prompt al ve Ã¼ret"""
    
    print("\n" + "="*60)
    print("LUNA-LM Ä°NTERAKTÄ°F MOD")
    print("="*60)
    print("\nKomutlar:")
    print("  - Metin girin ve Enter'a basÄ±n")
    print("  - 'quit' veya 'exit' yazarak Ã§Ä±kÄ±ÅŸ yapÄ±n")
    print("  - 'params' yazarak parametreleri deÄŸiÅŸtirin")
    print("="*60 + "\n")
    
    # VarsayÄ±lan parametreler
    params = {
        'max_tokens': 100,
        'temperature': 0.8,
        'top_k': 50
    }
    
    while True:
        try:
            prompt = input("\nğŸ“ Prompt: ").strip()
            
            if not prompt:
                continue
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("\nGÃ¶rÃ¼ÅŸmek Ã¼zere! ğŸ‘‹")
                break
            
            if prompt.lower() == 'params':
                print("\nMevcut parametreler:")
                print(f"  max_tokens: {params['max_tokens']}")
                print(f"  temperature: {params['temperature']}")
                print(f"  top_k: {params['top_k']}")
                
                try:
                    params['max_tokens'] = int(input("  Yeni max_tokens (Enter=deÄŸiÅŸmez): ") or params['max_tokens'])
                    params['temperature'] = float(input("  Yeni temperature (Enter=deÄŸiÅŸmez): ") or params['temperature'])
                    params['top_k'] = int(input("  Yeni top_k (Enter=deÄŸiÅŸmez): ") or params['top_k'])
                    print("  âœ“ Parametreler gÃ¼ncellendi!")
                except:
                    print("  âœ— GeÃ§ersiz deÄŸer, parametreler deÄŸiÅŸtirilmedi.")
                continue
            
            # Metin Ã¼ret
            print("\nğŸ¤– Luna-LM:")
            generated = generate_text(
                model, tokenizer, device, prompt,
                max_new_tokens=params['max_tokens'],
                temperature=params['temperature'],
                top_k=params['top_k']
            )
            print(generated)
            
        except KeyboardInterrupt:
            print("\n\nGÃ¶rÃ¼ÅŸmek Ã¼zere! ğŸ‘‹")
            break
        except Exception as e:
            print(f"\nâŒ Hata: {e}")


# ==================== MAIN ====================

def main():
    print("\n" + "="*60)
    print("LUNA-LM INFERENCE")
    print("="*60 + "\n")
    
    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}\n")
    
    # Proje kÃ¶k dizini
    project_root = os.path.join(os.path.dirname(__file__), '..')
    
    # Checkpoint dizini bul (Ã¶nce yeni yapÄ±, sonra eski yapÄ±)
    checkpoint_dirs = glob.glob(os.path.join(project_root, "checkpoints", "pretrain_*"))
    if not checkpoint_dirs:
        checkpoint_dirs = glob.glob(os.path.join(project_root, "luna_lm_checkpoints_*"))
    
    if not checkpoint_dirs:
        print("âŒ HiÃ§ checkpoint bulunamadÄ±!")
        print("   Ã–nce scripts/train.py ile model eÄŸitin.")
        return
    
    # En son checkpoint'i seÃ§
    checkpoint_dir = sorted(checkpoint_dirs)[-1]
    print(f"Checkpoint dizini: {checkpoint_dir}\n")
    
    # Modeli yÃ¼kle
    model, tokenizer, model_config = load_model(checkpoint_dir, device=device)
    
    # Test prompts
    print("\n" + "="*60)
    print("TEST ÃœRETÄ°MLERÄ°")
    print("="*60)
    
    test_prompts = [
        "BugÃ¼n hava Ã§ok gÃ¼zel",
        "Yapay zekÃ¢ teknolojisi",
        "Tarih boyunca insanlÄ±k",
        "Bilim ve teknoloji"
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ Prompt: '{prompt}'")
        print("ğŸ¤– Luna-LM:")
        generated = generate_text(
            model, tokenizer, device, prompt,
            max_new_tokens=80,
            temperature=0.8,
            top_k=50
        )
        print(generated)
    
    # Ä°nteraktif mod
    print("\n" + "="*60)
    use_interactive = input("\nÄ°nteraktif moda geÃ§mek ister misiniz? (y/n): ").strip().lower()
    
    if use_interactive == 'y':
        interactive_mode(model, tokenizer, device, model_config)
    else:
        print("\nBitti! Ä°nteraktif mod iÃ§in tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.")


if __name__ == "__main__":
    main()
