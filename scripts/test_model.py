"""
Luna-LM Test Script
EÄŸitilmiÅŸ modeli (pretrained veya SFT) test etmek iÃ§in
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import torch
import glob

from luna.utils import load_model
from luna.generate import generate_text


# SFT Prompt Format
SYSTEM_PROMPT = "Senin adÄ±n Luna. AmacÄ±n insanlara yardÄ±mcÄ± olmak ve sorulara aÃ§Ä±k, anlaÅŸÄ±lÄ±r cevaplar vermektir. Emin olmadÄ±ÄŸÄ±n konularda bunu belirtir, uydurma bilgi eklemezsin. CevaplarÄ±nÄ± nazik, sade ve doÄŸal bir TÃ¼rkÃ§e ile yazarsÄ±n."


def format_sft_prompt(user_query):
    return f"<system>{SYSTEM_PROMPT}</system>\n<user>{user_query}</user>\n<assistant>"


def clean_sft_output(generated):
    """Tokenizer artifactlerini temizle ve asistan cevabÄ±nÄ± Ã§Ä±kar"""
    clean_gen = generated.replace("< assistant >", "<assistant>").replace("< / assistant >", "</assistant>")
    clean_gen = clean_gen.replace("< user >", "<user>").replace("< / user >", "</user>")
    clean_gen = clean_gen.replace("< system >", "<system>").replace("< / system >", "</system>")
    
    # En son aÃ§Ä±lan <assistant> taginden sonrasÄ±nÄ± al
    if "<assistant>" in clean_gen:
        answer = clean_gen.split("<assistant>")[-1]
    else:
        answer = clean_gen
        
    # Stop token kontrolÃ¼
    for stop_token in ["</assistant>", "<user>", "<system>", "[SEP]"]:
        if stop_token in answer:
            answer = answer.split(stop_token)[0]
    
    # Gereksiz karakter temizliÄŸi
    answer = answer.strip()
    while answer and (answer[0] in ('>', ' ', '.')):
        answer = answer[1:].strip()
        
    return answer


def find_best_checkpoint(project_root):
    """En iyi modeli bul: Ã¶nce SFT, yoksa pretrained"""
    
    # 1. SFT checkpoint (en son)
    sft_dirs = sorted(glob.glob(os.path.join(project_root, "checkpoints", "sft_*")))
    if sft_dirs:
        latest_sft = sft_dirs[-1]
        best_model = os.path.join(latest_sft, "best_sft_model.pt")
        if os.path.exists(best_model):
            print(f"  âœ“ SFT model bulundu: {latest_sft}")
            return latest_sft, "sft"
    
    # 2. Pretrained checkpoint (yeni yapÄ±)
    pretrain_dirs = sorted(glob.glob(os.path.join(project_root, "checkpoints", "pretrain_*")))
    if pretrain_dirs:
        print(f"  âš  SFT model yok, pretrained kullanÄ±lacak")
        return pretrain_dirs[-1], "pretrained"
    
    # 3. Eski yapÄ±
    old_dirs = sorted(glob.glob(os.path.join(project_root, "luna_lm_checkpoints_*")))
    if old_dirs:
        print(f"  âš  SFT model yok, eski pretrained kullanÄ±lacak")
        return old_dirs[-1], "pretrained"
    
    return None, None


def main():
    print("="*60)
    print("LUNA-LM TEST")
    print("="*60)
    
    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # Proje kÃ¶k dizini
    project_root = os.path.join(os.path.dirname(__file__), '..')
    
    # Model bul ve yÃ¼kle
    print("\nğŸ“¦ Model aranÄ±yor...")
    checkpoint_path, model_type = find_best_checkpoint(project_root)
    
    if checkpoint_path is None:
        print("âŒ HiÃ§ model bulunamadÄ±!")
        print("   Ã–nce eÄŸitim yapÄ±n: python scripts/train.py")
        return
    
    print(f"  Model tipi: {model_type.upper()}")
    print(f"  Yol: {checkpoint_path}")
    
    try:
        model, tokenizer, config = load_model(checkpoint_path, device)
    except Exception as e:
        print(f"\nâš ï¸ Model yÃ¼klenirken hata: {e}")
        return
    
    # Test sorularÄ±
    print("\n" + "="*60)
    print("SFT METÄ°N ÃœRETÄ°MÄ° TESTÄ°")
    print("="*60)
    
    test_questions = [
        "GÃ¼neÅŸ hangi yÃ¶nden doÄŸar?",
        "AmpulÃ¼ kim buldu?",
        "Ä°stanbul'un Ã¶nemi nedir?",
        "Mutluluk nedir?",
        "Yapay zeka ne iÅŸe yarar?",
        "TÃ¼rkiye'nin baÅŸkenti neresidir?",
    ]
    
    for q in test_questions:
        print(f"\nâ“ {q}")
        
        full_prompt = format_sft_prompt(q)
        
        generated = generate_text(
            model, tokenizer, device,
            full_prompt, 
            max_new_tokens=150, 
            temperature=0.3,    
            top_k=40,
            repetition_penalty=1.2
        )
        
        answer = clean_sft_output(generated)
        print(f"ğŸ¤– {answer}")

    # Ä°nteraktif mod
    print("\n" + "="*60)
    print("Ä°NTERAKTÄ°F SOHBET (Ã‡Ä±kÄ±ÅŸ: q)")
    print("="*60)
    
    while True:
        try:
            user_input = input("\nâ“ Siz: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\nGÃ¶rÃ¼ÅŸÃ¼rÃ¼z! ğŸ‘‹")
            break
            
        if user_input.lower() in ('q', 'quit', 'exit'):
            print("GÃ¶rÃ¼ÅŸÃ¼rÃ¼z! ğŸ‘‹")
            break
        
        if not user_input:
            continue
        
        full_prompt = format_sft_prompt(user_input)
        
        generated = generate_text(
            model, tokenizer, device,
            full_prompt,
            max_new_tokens=150,
            temperature=0.7,
            top_k=50,
            repetition_penalty=1.2
        )
        
        answer = clean_sft_output(generated)
        print(f"ğŸ¤– Luna: {answer}")


if __name__ == "__main__":
    main()
