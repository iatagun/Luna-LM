{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåô Luna-LM Large ‚Äî H100 Training (v2)\n",
    "\n",
    "**Model:** Large (~350M params) ‚Äî emb_dim=1024, n_heads=16, n_layers=24  \n",
    "**GPU:** H100 (80GB VRAM)  \n",
    "**Pipeline:** Pretrain ‚Üí SFT\n",
    "\n",
    "### D√ºzeltmeler (v2)\n",
    "- `BATCH_SIZE`: 32 ‚Üí 8 (OOM d√ºzeltmesi)\n",
    "- `CONTEXT_LEN`: 1024 ‚Üí 512 (bellek tasarrufu)\n",
    "- `torch.compile` kaldƒ±rƒ±ldƒ± (OOM kaynaƒüƒ±)\n",
    "- `autocast` deprecated uyarƒ±sƒ± d√ºzeltildi\n",
    "- `gradient_accumulation_steps=4` eklendi (etkin batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GPU Doƒürula\n",
    "import torch\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "print(f'BF16 destekli: {torch.cuda.is_bf16_supported()}')\n",
    "assert torch.cuda.is_available(), '‚ùå GPU yok! Runtime > Change runtime type > H100'\n",
    "print('‚úÖ GPU hazƒ±r!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Google Drive Baƒüla\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_DIR = '/content/drive/MyDrive/LunaLM'\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print(f'‚úÖ Drive: {DRIVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Repo Kur\n",
    "import sys, os\n",
    "if not os.path.exists('/content/Luna-LM'):\n",
    "    !git clone https://github.com/iatagun/Luna-LM.git /content/Luna-LM\n",
    "else:\n",
    "    print('Repo zaten mevcut, g√ºncelleniyor...')\n",
    "    !cd /content/Luna-LM && git pull\n",
    "\n",
    "sys.path.insert(0, '/content/Luna-LM')\n",
    "os.chdir('/content/Luna-LM')\n",
    "print('‚úÖ Repo hazƒ±r!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Baƒüƒ±mlƒ±lƒ±klarƒ± Y√ºkle\n",
    "!pip install transformers datasets\n",
    "\n",
    "# Kurulumu doƒürula\n",
    "import transformers, datasets as ds_lib\n",
    "print(f'‚úÖ transformers {transformers.__version__}')\n",
    "print(f'‚úÖ datasets     {ds_lib.__version__}')\n",
    "\n",
    "# Uyarƒ±larƒ± bastƒ±r\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Token indices sequence length')\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Corpus Y√ºkle\n",
    "CORPUS_DRIVE = '/content/drive/MyDrive/LunaLM/foundation_corpus_clean.txt'\n",
    "CORPUS_LOCAL = '/content/Luna-LM/foundation_corpus_clean.txt'\n",
    "\n",
    "if not os.path.exists(CORPUS_LOCAL):\n",
    "    if os.path.exists(CORPUS_DRIVE):\n",
    "        print('Corpus kopyalanƒ±yor...')\n",
    "        import shutil\n",
    "        shutil.copy(CORPUS_DRIVE, CORPUS_LOCAL)\n",
    "    else:\n",
    "        print('‚ùå Corpus bulunamadƒ±! Drive\\'a y√ºkleyin:')\n",
    "        print(f'   {CORPUS_DRIVE}')\n",
    "\n",
    "size_gb = os.path.getsize(CORPUS_LOCAL) / 1024**3\n",
    "print(f'‚úÖ Corpus: {size_gb:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. AYARLAR ‚Äî OOM d√ºzeltmeleri uygulandƒ±\n",
    "import torch, json, random, time, math, os\n",
    "from datetime import datetime\n",
    "\n",
    "from luna.tokenizer import PretrainedTurkishTokenizer\n",
    "from luna.data import create_dataloader_pretrained\n",
    "from luna.model import GPTModel, MODEL_CONFIGS\n",
    "from luna.generate import generate_text\n",
    "\n",
    "# ==========================================\n",
    "# AYARLAR\n",
    "# ==========================================\n",
    "MODEL_SIZE  = 'large'   # emb_dim=1024, n_heads=16, n_layers=24\n",
    "BATCH_SIZE  = 8         # ‚úÖ 32‚Üí8 (OOM d√ºzeltmesi)\n",
    "GRAD_ACCUM  = 4         # Gradient accumulation: etkin batch = 8*4 = 32\n",
    "CONTEXT_LEN = 512       # ‚úÖ 1024‚Üí512 (bellek tasarrufu)\n",
    "NUM_EPOCHS  = 3\n",
    "LR          = 6e-4\n",
    "WD          = 0.1\n",
    "EVAL_FREQ   = 500\n",
    "EVAL_ITER   = 10\n",
    "MAX_LINES   = None      # T√ºm corpus\n",
    "USE_BF16    = True\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(f'Device: {device}')\n",
    "print(f'Model:  {MODEL_SIZE}')\n",
    "print(f'Batch:  {BATCH_SIZE} x {GRAD_ACCUM} accum = {BATCH_SIZE*GRAD_ACCUM} etkin')\n",
    "print(f'Ctx:    {CONTEXT_LEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = PretrainedTurkishTokenizer('dbmdz/bert-base-turkish-cased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f'‚úÖ Vocab: {vocab_size:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus Y√ºkle & Split\n",
    "corpus_path = CORPUS_LOCAL\n",
    "print('Corpus y√ºkleniyor...')\n",
    "\n",
    "lines = []\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if MAX_LINES and i >= MAX_LINES:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "        if (i+1) % 500_000 == 0:\n",
    "            print(f'  {i+1:,} satƒ±r...')\n",
    "\n",
    "print(f'‚úÖ {len(lines):,} satƒ±r')\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(lines)\n",
    "\n",
    "split      = int(0.95 * len(lines))\n",
    "train_text = '\\n'.join(lines[:split])\n",
    "val_text   = '\\n'.join(lines[split:])\n",
    "del lines\n",
    "\n",
    "print(f'Train: {len(train_text)/1e9:.2f}B karakter')\n",
    "print(f'Val:   {len(val_text)/1e6:.1f}M karakter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "print('Tokenize ediliyor (corpus b√ºy√ºk, ~20-30 dk)...')\n",
    "\n",
    "train_loader = create_dataloader_pretrained(\n",
    "    train_text, tokenizer,\n",
    "    batch_size=BATCH_SIZE, max_length=CONTEXT_LEN, stride=CONTEXT_LEN, shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader_pretrained(\n",
    "    val_text, tokenizer,\n",
    "    batch_size=BATCH_SIZE, max_length=CONTEXT_LEN, stride=CONTEXT_LEN, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Train: {len(train_loader):,} batch')\n",
    "print(f'‚úÖ Val:   {len(val_loader):,} batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Olu≈ütur\n",
    "cfg = MODEL_CONFIGS[MODEL_SIZE]\n",
    "model_config = {\n",
    "    'vocab_size':     vocab_size,\n",
    "    'context_length': CONTEXT_LEN,\n",
    "    'emb_dim':        cfg['emb_dim'],\n",
    "    'n_heads':        cfg['n_heads'],\n",
    "    'n_layers':       cfg['n_layers'],\n",
    "    'drop_rate':      0.1,\n",
    "    'qkv_bias':       False,\n",
    "}\n",
    "\n",
    "model = GPTModel(model_config)\n",
    "\n",
    "if USE_BF16:\n",
    "    model = model.to(dtype=torch.bfloat16)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# ‚úÖ torch.compile KALDIRILDI ‚Äî OOM kaynaƒüƒ±ydƒ±\n",
    "# model = torch.compile(model)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'‚úÖ Model: {total/1e6:.1f}M parametre')\n",
    "print(f'   emb={cfg[\"emb_dim\"]}, heads={cfg[\"n_heads\"]}, layers={cfg[\"n_layers\"]}')\n",
    "\n",
    "# VRAM kullanƒ±mƒ±nƒ± g√∂ster\n",
    "allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "reserved  = torch.cuda.memory_reserved()  / 1024**3\n",
    "print(f'   VRAM: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & Scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR,\n",
    "    weight_decay=WD,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-8,\n",
    "    fused=True\n",
    ")\n",
    "\n",
    "total_steps  = (len(train_loader) // GRAD_ACCUM) * NUM_EPOCHS\n",
    "warmup_steps = int(total_steps * 0.02)\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(warmup_steps, 1)\n",
    "    progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "    return 0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(f'‚úÖ Optimizer hazƒ±r')\n",
    "print(f'   Total steps: {total_steps:,} ({warmup_steps} warmup)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dir\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_dir  = f'{DRIVE_DIR}/pretrain_large_{timestamp}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(f'{save_dir}/config.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'model_config': model_config,\n",
    "        'training_config': {\n",
    "            'model_size': MODEL_SIZE,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'grad_accum': GRAD_ACCUM,\n",
    "            'effective_batch': BATCH_SIZE * GRAD_ACCUM,\n",
    "            'num_epochs': NUM_EPOCHS,\n",
    "            'learning_rate': LR,\n",
    "            'context_length': CONTEXT_LEN,\n",
    "            'use_bf16': USE_BF16,\n",
    "        },\n",
    "        'tokenizer': 'dbmdz/bert-base-turkish-cased',\n",
    "        'timestamp': timestamp,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Save dir: {save_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETRAIN LOOP ‚Äî Gradient Accumulation + BF16\n",
    "# ‚úÖ torch.cuda.amp.autocast ‚Üí torch.amp.autocast (deprecated uyarƒ±sƒ± d√ºzeltildi)\n",
    "\n",
    "train_losses, val_losses, tokens_log = [], [], []\n",
    "tokens_seen   = 0\n",
    "global_step   = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print('='*60)\n",
    "print('PRETRAIN BA≈ûLIYOR')\n",
    "print(f'Grad Accum: {GRAD_ACCUM} ‚Üí Etkin batch: {BATCH_SIZE*GRAD_ACCUM}')\n",
    "print('='*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    ep_start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (input_batch, target_batch) in enumerate(train_loader):\n",
    "        input_batch  = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "\n",
    "        # ‚úÖ Yeni autocast API\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16, enabled=USE_BF16):\n",
    "            logits = model(input_batch)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.flatten(0, 1), target_batch.flatten()\n",
    "            )\n",
    "            loss = loss / GRAD_ACCUM  # Scale loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (batch_idx + 1) % GRAD_ACCUM == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            tokens_seen += input_batch.numel() * GRAD_ACCUM\n",
    "\n",
    "            # Eval\n",
    "            if global_step % EVAL_FREQ == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    tl = 0\n",
    "                    for k, (ib, tb) in enumerate(train_loader):\n",
    "                        if k >= EVAL_ITER: break\n",
    "                        with torch.amp.autocast('cuda', dtype=torch.bfloat16, enabled=USE_BF16):\n",
    "                            lg = model(ib.to(device))\n",
    "                            tl += torch.nn.functional.cross_entropy(lg.flatten(0,1), tb.to(device).flatten()).item()\n",
    "                    tl /= EVAL_ITER\n",
    "\n",
    "                    vl = 0\n",
    "                    for k, (ib, tb) in enumerate(val_loader):\n",
    "                        if k >= EVAL_ITER: break\n",
    "                        with torch.amp.autocast('cuda', dtype=torch.bfloat16, enabled=USE_BF16):\n",
    "                            lg = model(ib.to(device))\n",
    "                            vl += torch.nn.functional.cross_entropy(lg.flatten(0,1), tb.to(device).flatten()).item()\n",
    "                    vl /= EVAL_ITER\n",
    "\n",
    "                train_losses.append(tl)\n",
    "                val_losses.append(vl)\n",
    "                tokens_log.append(tokens_seen)\n",
    "\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                mem = torch.cuda.memory_allocated() / 1024**3\n",
    "                print(f'Ep {epoch+1} | Step {global_step:,} | '\n",
    "                      f'Train: {tl:.4f} | Val: {vl:.4f} | '\n",
    "                      f'LR: {lr:.2e} | Tokens: {tokens_seen/1e6:.0f}M | VRAM: {mem:.1f}GB')\n",
    "\n",
    "                if vl < best_val_loss:\n",
    "                    best_val_loss = vl\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'epoch': epoch, 'step': global_step,\n",
    "                        'val_loss': vl, 'tokens_seen': tokens_seen,\n",
    "                    }, f'{save_dir}/best_model.pt')\n",
    "                    print(f'  ‚úÖ Best! Val: {vl:.4f}')\n",
    "\n",
    "                model.train()\n",
    "\n",
    "    ep_time = (time.time() - ep_start) / 60\n",
    "    print(f'\\nEpoch {epoch+1} tamamlandƒ± ({ep_time:.0f} dk)\\n')\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch},\n",
    "               f'{save_dir}/epoch_{epoch+1}.pt')\n",
    "\n",
    "print(f'‚úÖ PRETRAIN TAMAMLANDI! Best val: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Grafiƒüi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(tokens_log, train_losses, label='Train')\n",
    "ax.plot(tokens_log, val_losses, label='Val', linestyle='--')\n",
    "ax.set_xlabel('Tokens Seen')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Luna-LM Large ‚Äî Pretrain Loss')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{save_dir}/pretrain_loss.png', dpi=150)\n",
    "plt.show()\n",
    "print(f'‚úÖ Grafik kaydedildi: {save_dir}/pretrain_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SFT ‚Äî Alpaca Turkish (82K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Dataset ƒ∞ndir\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "SFT_PATH = '/content/Luna-LM/sft/sft_dataset.jsonl'\n",
    "\n",
    "if not os.path.exists(SFT_PATH):\n",
    "    print('SFT dataset indiriliyor...')\n",
    "    ds = load_dataset('cenfis/alpaca-turkish-combined')\n",
    "    valid = 0\n",
    "    with open(SFT_PATH, 'w', encoding='utf-8') as f:\n",
    "        for row in ds['train']:\n",
    "            user = row['instruction']\n",
    "            if row.get('input'):\n",
    "                user += f\"\\n{row['input']}\"\n",
    "            if user and row['output']:\n",
    "                f.write(json.dumps(\n",
    "                    {'user': user, 'assistant': row['output']},\n",
    "                    ensure_ascii=False\n",
    "                ) + '\\n')\n",
    "                valid += 1\n",
    "    print(f'‚úÖ SFT: {valid:,} √∂rnek ‚Üí {SFT_PATH}')\n",
    "else:\n",
    "    print(f'‚úÖ SFT dataset mevcut: {SFT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT i√ßin batch ayarla (train_sft.py'deki deƒüeri ge√ßersiz kƒ±l)\n",
    "# train_sft.py'yi H100 i√ßin patch et\n",
    "import re\n",
    "\n",
    "sft_script = '/content/Luna-LM/sft/train_sft.py'\n",
    "with open(sft_script, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# BATCH_SIZE'ƒ± 8'e, EVAL_FREQ'i 200'e ayarla\n",
    "content = re.sub(r'BATCH_SIZE\\s*=\\s*\\d+', 'BATCH_SIZE = 8', content)\n",
    "content = re.sub(r'EVAL_FREQ\\s*=\\s*\\d+', 'EVAL_FREQ = 200', content)\n",
    "\n",
    "with open(sft_script, 'w', encoding='utf-8') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print('‚úÖ train_sft.py g√ºncellendi (BATCH=8, EVAL_FREQ=200)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Eƒüitimi √áalƒ±≈ütƒ±r\n",
    "!python sft/train_sft.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her ≈üeyi Drive'a Kaydet\n",
    "import glob, shutil\n",
    "\n",
    "# SFT checkpoints\n",
    "sft_dirs = sorted(glob.glob('/content/Luna-LM/checkpoints/sft_*'))\n",
    "if sft_dirs:\n",
    "    dest = os.path.join(DRIVE_DIR, os.path.basename(sft_dirs[-1]))\n",
    "    shutil.copytree(sft_dirs[-1], dest, dirs_exist_ok=True)\n",
    "    print(f'‚úÖ SFT Drive\\'a kopyalandƒ±: {dest}')\n",
    "\n",
    "print('‚úÖ Tamamlandƒ±!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hƒ±zlƒ± Test\n",
    "from luna.utils import load_model\n",
    "from luna.generate import generate_text\n",
    "import glob\n",
    "\n",
    "SYSTEM = ('Senin adƒ±n Luna. Amacƒ±n insanlara yardƒ±mcƒ± olmak ve sorulara a√ßƒ±k, '\n",
    "          'anla≈üƒ±lƒ±r cevaplar vermektir.')\n",
    "\n",
    "def chat(model, tok, dev, q):\n",
    "    prompt = f'<system>{SYSTEM}</system>\\n<user>{q}</user>\\n<assistant>'\n",
    "    out = generate_text(model, tok, dev, prompt,\n",
    "                        max_new_tokens=150, temperature=0.7,\n",
    "                        top_k=50, repetition_penalty=1.2)\n",
    "    if '<assistant>' in out:\n",
    "        ans = out.split('<assistant>')[-1]\n",
    "        for s in ['</assistant>', '<user>', '<system>', '[SEP]']:\n",
    "            ans = ans.split(s)[0]\n",
    "        return ans.strip()\n",
    "    return out\n",
    "\n",
    "# En son SFT checkpoint\n",
    "sft_dirs = sorted(glob.glob('/content/Luna-LM/checkpoints/sft_*'))\n",
    "model_t, tok_t, _ = load_model(sft_dirs[-1], device)\n",
    "\n",
    "for q in ['G√ºne≈ü hangi y√∂nden doƒüar?', 'Yapay zeka nedir?', 'T√ºrkiye\\'nin ba≈ükenti?']:\n",
    "    print(f'‚ùì {q}')\n",
    "    print(f'ü§ñ {chat(model_t, tok_t, device, q)}\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "H100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
