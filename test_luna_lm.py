"""
Luna-LM Test Script
En son eÄŸitilen modeli test etmek iÃ§in
"""

import torch
import json
import os

from model import GPTModel, generate_text
from turkish_tokenizer_pretrained import PretrainedTurkishTokenizer


def load_model(checkpoint_path, device='cuda'):
    """Model ve tokenizer'Ä± yÃ¼kle. checkpoint_path bir klasÃ¶r VEYA .pt dosyasÄ± olabilir."""
    
    model_config = None
    tokenizer_name = 'dbmdz/bert-base-turkish-cased'  # VarsayÄ±lan
    
    # Durum 1: checkpoint_path bir KLASÃ–R (Pretraining Ã§Ä±ktÄ±sÄ±)
    if os.path.isdir(checkpoint_path):
        config_path = os.path.join(checkpoint_path, "config.json")
        weights_path = os.path.join(checkpoint_path, "best_model.pt")
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            model_config = config['model_config']
            tokenizer_name = config.get('tokenizer', tokenizer_name)
    
    # Durum 2: checkpoint_path bir DOSYA (.pt - SFT Ã§Ä±ktÄ±sÄ±)
    elif os.path.isfile(checkpoint_path):
        weights_path = checkpoint_path
        
        base_dir = os.path.dirname(checkpoint_path) or '.'
        config_path = os.path.join(base_dir, "config.json")
        if os.path.exists(config_path):
             with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                model_config = config['model_config']
        else:
             print("UYARI: Config dosyasÄ± bulunamadÄ±, varsayÄ±lan 'small' config kullanÄ±lÄ±yor.")
             model_config = {
                "vocab_size": 32000,
                "context_length": 512,
                "emb_dim": 512,
                "n_heads": 8,
                "n_layers": 6,
                "drop_rate": 0.1,
                "qkv_bias": False
            }
    else:
        raise ValueError(f"GeÃ§ersiz yol: {checkpoint_path}")

    tokenizer = PretrainedTurkishTokenizer(tokenizer_name)
    model_config['vocab_size'] = tokenizer.vocab_size

    print(f"Model config: {model_config}")
    
    model = GPTModel(model_config)
    
    print(f"AÄŸÄ±rlÄ±klar yÃ¼kleniyor: {weights_path}")
    checkpoint = torch.load(weights_path, map_location=device)
    
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    else:
        state_dict = checkpoint
        
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()
    
    return model, tokenizer, model_config


def main():
    print("="*60)
    print("LUNA-LM TEST")
    print("="*60)
    
    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # Model yÃ¼kle
    # Ã–nce SFT modeline bak, yoksa klasÃ¶re bak
    if os.path.exists("luna_sft_finetuned.pt"):
        checkpoint_path = "luna_sft_finetuned.pt"
    else:
        checkpoint_path = "luna_lm_checkpoints_20251218_121142"
        
    print(f"\nğŸ“¦ Model Yolu: {checkpoint_path}")
    
    try:
        model, tokenizer, config = load_model(checkpoint_path, device)
    except Exception as e:
        print(f"\nâš ï¸ Model yÃ¼klenirken hata oluÅŸtu: {e}")
        print(f"LÃ¼tfen 'luna_sft_finetuned.pt' dosyasÄ±nÄ±n bu klasÃ¶rde olduÄŸundan emin olun.")
        return
    
    # SFT Prompt Format
    SYSTEM_PROMPT = "Senin adÄ±n Luna. AmacÄ±n insanlara yardÄ±mcÄ± olmak ve sorulara aÃ§Ä±k, anlaÅŸÄ±lÄ±r cevaplar vermektir. Emin olmadÄ±ÄŸÄ±n konularda bunu belirtir, uydurma bilgi eklemezsin. CevaplarÄ±nÄ± nazik, sade ve doÄŸal bir TÃ¼rkÃ§e ile yazarsÄ±n."
    
    def format_sft_prompt(user_query):
        return f"<system>{SYSTEM_PROMPT}</system>\n<user>{user_query}</user>\n<assistant>"

    # Test prompts (SFT iÃ§in Soru FormatÄ±nda)
    print("\n" + "="*60)
    print("SFT FORMATLI METÄ°N ÃœRETÄ°MÄ° TESTÄ°")
    print("="*60)
    
    test_questions = [
        "GÃ¼neÅŸ hangi yÃ¶nden doÄŸar?", # Dataset'te VAR
        "AmpulÃ¼ kim buldu?",         # Dataset'te VAR
        "Ä°stanbul'un Ã¶nemi nedir?",  # Dataset'te YOK (Benzeri var ama aynÄ±sÄ± deÄŸil)
        "Mutluluk nedir?",           # Dataset'te VAR
        "Bana bir hikaye anlat.",    # Dataset'te VAR
        "Kravat nasÄ±l baÄŸlanÄ±r?",    # Dataset'te VAR
    ]
    
    for q in test_questions:
        print(f"\nâ“ Soru: '{q}'")
        print("-" * 40)
        
        full_prompt = format_sft_prompt(q)
        
        # Temperature'Ä± dÃ¼ÅŸÃ¼rdÃ¼m (0.2). Model kÃ¼Ã§Ã¼k olduÄŸu iÃ§in yaratÄ±cÄ±lÄ±k = saÃ§malama oluyor.
        generated = generate_text(
            model, tokenizer, device,
            full_prompt, 
            max_new_tokens=100, 
            temperature=0.2,    
            top_k=40,
            repetition_penalty=1.2 
        )
        
        # === AKILLI TEMÄ°ZLÄ°K (Tokenizer artifactlerini temizle) ===
        # Tokenizer < system > ÅŸeklinde boÅŸluklu Ã¼retebiliyor, bunlarÄ± normalleÅŸtirelim
        clean_gen = generated.replace("< assistant >", "<assistant>").replace("< / assistant >", "</assistant>")
        clean_gen = clean_gen.replace("< user >", "<user>").replace("< / user >", "</user>")
        clean_gen = clean_gen.replace("< system >", "<system>").replace("< / system >", "</system>")
        
        # En son aÃ§Ä±lan <assistant> taginden sonrasÄ±nÄ± al (Cevap oradadÄ±r)
        if "<assistant>" in clean_gen:
             answer = clean_gen.split("<assistant>")[-1]
        else:
             answer = clean_gen
            
        # Stop token kontrolÃ¼ (CevabÄ±n bittiÄŸi yer)
        for stop_token in ["</assistant>", "<user>", "<system>"]:
            if stop_token in answer:
                answer = answer.split(stop_token)[0]
        
        # Gereksiz karakter temizliÄŸi
        answer = answer.strip()
        while answer.startswith(">") or answer.startswith(" ") or answer.startswith("."):
            answer = answer[1:].strip()
            
        print(f"ğŸ¤– Luna: {answer}")

    # Ä°nteraktif mod
    print("\n" + "="*60)
    print("Ä°NTERAKTÄ°F SOHBET MODU (Ã‡Ä±kÄ±ÅŸ iÃ§in 'q')")
    print("="*60)
    
    while True:
        user_input = input("\nSiz: ").strip()
        if user_input.lower() == 'q':
            print("GÃ¶rÃ¼ÅŸÃ¼rÃ¼z! ğŸ‘‹")
            break
        
        if not user_input:
            continue
        
        full_prompt = format_sft_prompt(user_input)
        
        generated = generate_text(
            model, tokenizer, device,
            full_prompt,
            max_new_tokens=150,
            temperature=0.7,
            top_k=50,
            repetition_penalty=1.2
        )
        
        # === AYNI TEMÄ°ZLÄ°K MANTIÄI ===
        clean_gen = generated.replace("< assistant >", "<assistant>").replace("< / assistant >", "</assistant>")
        clean_gen = clean_gen.replace("< user >", "<user>").replace("< / user >", "</user>")
        clean_gen = clean_gen.replace("< system >", "<system>").replace("< / system >", "</system>")
        
        if "<assistant>" in clean_gen:
             answer = clean_gen.split("<assistant>")[-1]
        else:
             answer = clean_gen
            
        for stop_token in ["</assistant>", "<user>", "<system>"]:
            if stop_token in answer:
                answer = answer.split(stop_token)[0]
        
        answer = answer.strip()
        while answer.startswith(">") or answer.startswith(" ") or answer.startswith("."):
             answer = answer[1:].strip()

        print(f"Luna: {answer}")


if __name__ == "__main__":
    main()
