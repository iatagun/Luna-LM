# Luna-LM: TÃ¼rkÃ§e Foundation Language Model ğŸŒ™

SÄ±fÄ±rdan TÃ¼rkÃ§e dil modeli eÄŸitimi - GPT mimarisi ile PyTorch implementasyonu.

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# 1. Gereksinimleri yÃ¼kle
pip install torch transformers tokenizers matplotlib

# 2. Model eÄŸitimini baÅŸlat (2-4 saat)
python train_luna_lm.py

# 3. EÄŸitilmiÅŸ modeli kullan
python inference_luna_lm.py
```

## ğŸ“– DokÃ¼mantasyon

- **[QUICKSTART.md](QUICKSTART.md)** - HÄ±zlÄ± baÅŸlangÄ±Ã§ rehberi
- **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)** - DetaylÄ± eÄŸitim rehberi  
- **[TURKISH_TOKENIZER_README.md](TURKISH_TOKENIZER_README.md)** - Tokenizer kullanÄ±mÄ±

## ğŸ¯ Ã–zellikler

- âœ… TÃ¼rkÃ§e corpus Ã¼zerinde pretraining (foundation_corpus.txt)
- âœ… 3 farklÄ± model boyutu (10M, 50M, 150M parametre)
- âœ… Pretrained TÃ¼rkÃ§e tokenizer (32K vocab)
- âœ… Custom tokenizer eÄŸitimi desteÄŸi
- âœ… Real-time training monitoring
- âœ… Ä°nteraktif metin Ã¼retimi
- âœ… Checkpoint sistemi
- âœ… GPU & CPU desteÄŸi

## ğŸ“Š Model BoyutlarÄ±

| Boyut | Parametreler | EÄŸitim SÃ¼resi | KullanÄ±m |
|-------|--------------|---------------|----------|
| tiny  | ~10M | 30-60 dk | HÄ±zlÄ± test |
| small | ~50M | 2-4 saat | âœ… Ã–nerilen |
| medium | ~150M | 6-12 saat | Ä°yi sonuÃ§ |

## ğŸ”§ Proje YapÄ±sÄ±

```
Luna-LM/
â”œâ”€â”€ model.py                            # â­ Model mimarisi (GPTModel, generate_text)
â”œâ”€â”€ train_luna_lm.py                    # Foundation model eÄŸitimi
â”œâ”€â”€ test_luna_lm.py                     # Model testi & interaktif sohbet
â”œâ”€â”€ inference_luna_lm.py                # Model inference
â”œâ”€â”€ sft_luna_lm.py                      # Supervised Fine-Tuning
â”‚
â”œâ”€â”€ generate_massive_sft.py             # SFT veri seti Ã¼retimi
â”‚
â”œâ”€â”€ turkish_tokenizer_pretrained.py     # HazÄ±r tokenizer wrapper
â”œâ”€â”€ turkish_tokenizer_huggingface.py    # Custom HF tokenizer
â”œâ”€â”€ turkish_tokenizer_training.py       # BPE sÄ±fÄ±rdan eÄŸitimi
â”‚
â”œâ”€â”€ requirements.txt                    # BaÄŸÄ±mlÄ±lÄ±klar
â”œâ”€â”€ QUICKSTART.md                       # HÄ±zlÄ± baÅŸlangÄ±Ã§
â”œâ”€â”€ TRAINING_GUIDE.md                   # DetaylÄ± rehber
â””â”€â”€ TURKISH_TOKENIZER_README.md         # Tokenizer rehberi
```


## ğŸ’¡ KullanÄ±m Ã–rneÄŸi

```python
from inference_luna_lm import load_model, generate_text
import torch

# Model yÃ¼kle
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model, tokenizer, config = load_model("luna_lm_checkpoints_XXXXXXXX", device=device)

# Metin Ã¼ret
text = generate_text(
    model, tokenizer, device,
    prompt="Yapay zekÃ¢",
    max_new_tokens=100,
    temperature=0.8
)
print(text)
```

## ğŸ“ˆ Beklenen SonuÃ§lar

10 epoch sonrasÄ±:
- Train Loss: 3.0-3.5
- Val Loss: 3.2-3.8
- TÃ¼rkÃ§e kelime ve cÃ¼mleler
- Temel gramer kurallarÄ±
- AnlamlÄ± metin Ã¼retimi

## ğŸ“ Sonraki AdÄ±mlar

1. **Fine-tuning**: Ã–zel gÃ¶revler iÃ§in model ince ayarÄ±
2. **Veri ArtÄ±rma**: Daha fazla TÃ¼rkÃ§e metin ekleme
3. **Model BÃ¼yÃ¼tme**: Daha bÃ¼yÃ¼k model boyutlarÄ±
4. **Deployment**: API servisi ve web arayÃ¼zÃ¼

## ğŸ™ TeÅŸekkÃ¼rler

- [Sebastian Raschka](https://sebastianraschka.com/) - LLMs from Scratch
- [DBMDz](https://huggingface.co/dbmdz) - TÃ¼rkÃ§e BERT tokenizer
- [Hugging Face](https://huggingface.co/) - Transformers kÃ¼tÃ¼phanesi