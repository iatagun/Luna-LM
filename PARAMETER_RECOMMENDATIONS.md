# ğŸ¯ Luna-LM Parametre Ã–nerileri

## ğŸ“Š Corpus Analizi

### Ä°statistikler
```
Toplam SatÄ±r:       25,832
Toplam Kelime:      506,744
Toplam Token:       839,275 (~840K)
Unique Kelime:      112,246
Karakter/Token:     4.55
Ortalama SatÄ±r:     147.7 karakter
Ortalama Kelime:    6.5 karakter
```

### Ä°Ã§erik DaÄŸÄ±lÄ±mÄ±
- âœ… **Ã‡ok Ã§eÅŸitli iÃ§erik**: Felsefe, bilim, AI, fizik, tarih, psikoloji, diyaloglar
- âœ… **DoÄŸal TÃ¼rkÃ§e**: GÃ¼nlÃ¼k konuÅŸmalar, blog tarzÄ±, formal bilimsel metin
- âœ… **Zengin kelime daÄŸarcÄ±ÄŸÄ±**: 112K unique kelime (Ã§ok iyi!)
- âœ… **Uzun baÄŸlamlar**: Ortalama 580 token/satÄ±r

---

## âš™ï¸ Optimized Parametreler

### **Ã–NCE (Eski Parametreler)**
```python
MODEL_SIZE = "small"        # 50M params
BATCH_SIZE = 4
CONTEXT_LENGTH = 256
NUM_EPOCHS = 10
LEARNING_RATE = 3e-4
EVAL_FREQ = 100
EVAL_ITER = 10
stride = CONTEXT_LENGTH // 2  # 128
```

### **SONRA (Yeni Parametreler)** âœ…
```python
MODEL_SIZE = "small"        # 50M params (aynÄ±)
BATCH_SIZE = 2              # â†“ GTX 1650 iÃ§in gÃ¼venli
CONTEXT_LENGTH = 512        # â†‘ Daha uzun baÄŸlam
NUM_EPOCHS = 15             # â†‘ KÃ¼Ã§Ã¼k veri iÃ§in daha fazla epoch
LEARNING_RATE = 5e-4        # â†‘ Daha agresif baÅŸlangÄ±Ã§
EVAL_FREQ = 50              # â†“ Daha sÄ±k deÄŸerlendirme
EVAL_ITER = 20              # â†‘ Daha iyi loss tahmini
stride = CONTEXT_LENGTH * 3 // 4  # 384 (daha fazla overlap)
```

### **Yeni Eklenen: Learning Rate Scheduler** ğŸ†•
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, 
    T_max=num_training_steps,
    eta_min=LEARNING_RATE * 0.1  # Son LR = 5e-5
)
```

---

## ğŸ“ˆ DeÄŸiÅŸiklik GerekÃ§eleri

### 1. **CONTEXT_LENGTH: 256 â†’ 512**
**Neden?**
- Corpus'ta ortalama satÄ±r ~580 token
- 256 ile uzun cÃ¼mleleri kesiyorduk
- 512 ile tam cÃ¼mleleri Ã¶ÄŸrenebilir

**Etki:**
- âœ… Daha iyi baÄŸlam anlama
- âœ… Uzun metinleri doÄŸru modelleyebilme
- âš ï¸ Daha fazla GPU memory

### 2. **BATCH_SIZE: 4 â†’ 2**
**Neden?**
- GTX 1650 = 4GB VRAM
- Context 512 olunca memory tÃ¼ketimi artar
- Batch 2 gÃ¼venli, training kararlÄ±

**Etki:**
- âœ… Out of memory riski yok
- âš ï¸ Training biraz daha yavaÅŸ (ama gÃ¼venli)

### 3. **NUM_EPOCHS: 10 â†’ 15**
**Neden?**
- 840K token kÃ¼Ã§Ã¼k bir corpus
- Daha fazla epoch = daha iyi Ã¶ÄŸrenme
- Overfitting riski dÃ¼ÅŸÃ¼k (veri Ã§eÅŸitli)

**Etki:**
- âœ… Model daha iyi Ã¶ÄŸrenir
- âš ï¸ Training sÃ¼resi %50 artar

### 4. **LEARNING_RATE: 3e-4 â†’ 5e-4 + Scheduler**
**Neden?**
- Daha yÃ¼ksek baÅŸlangÄ±Ã§ LR = hÄ±zlÄ± Ã¶ÄŸrenme
- Cosine scheduler = yumuÅŸak dÃ¼ÅŸÃ¼ÅŸ (3e-4 â†’ 5e-5)
- Overfitting'i Ã¶nler

**Etki:**
- âœ… Daha hÄ±zlÄ± convergence
- âœ… Daha stabil training
- âœ… Loss grafiÄŸi daha dÃ¼zgÃ¼n

### 5. **EVAL_FREQ: 100 â†’ 50**
**Neden?**
- 840K token ile ~600 step/epoch
- Her 50 step = epoch baÅŸÄ±na 12 evaluation
- Daha iyi training monitoring

**Etki:**
- âœ… Loss deÄŸiÅŸimlerini erken fark edersin
- âœ… Overfitting'i hemen gÃ¶rebilirsin
- âš ï¸ Biraz daha yavaÅŸ (ama deÄŸer)

### 6. **Stride: 128 â†’ 384 (3/4 overlap)**
**Neden?**
- Daha fazla overlap = daha fazla training sample
- Model aynÄ± metni farklÄ± pozisyonlardan gÃ¶rÃ¼r
- Data augmentation etkisi

**Etki:**
- âœ… ~2x daha fazla training sample
- âœ… Daha iyi generalization

---

## ğŸ• Beklenen Training SÃ¼resi

### Hesaplama:
```
Batch size: 2
Context: 512
Tokens per batch: 2 * 512 = 1,024
Total tokens: 840K
Batches per epoch: ~1,200 (stride ile artÄ±ÅŸ)
Total batches: 1,200 * 15 = 18,000
```

### GTX 1650 ile Tahmini SÃ¼re:
- **Batch/s**: ~0.5-1 (512 context ile)
- **Epoch sÃ¼resi**: 20-40 dakika
- **Total training**: **5-10 saat**

### Loss Beklentileri:
| Epoch | Train Loss | Val Loss |
|-------|-----------|----------|
| 1     | 6.5-7.0   | 6.8-7.2  |
| 5     | 3.5-4.0   | 3.8-4.2  |
| 10    | 2.8-3.2   | 3.0-3.5  |
| 15    | 2.5-2.8   | 2.8-3.2  |

---

## ğŸš€ EÄŸitimi BaÅŸlatma

```bash
python train_luna_lm.py
```

### EÄŸitim SÄ±rasÄ±nda Ä°zle:
1. **Loss deÄŸerleri**: Train < Val olmalÄ± (ama Ã§ok fark olmamalÄ±)
2. **Learning Rate**: Her step'te yavaÅŸ dÃ¼ÅŸmeli (5e-4 â†’ 5e-5)
3. **Generated text**: Her 250 step'te Ã¶rnek Ã¼retim
4. **GPU memory**: `nvidia-smi` ile kontrol et

---

## ğŸ“ Ä°yileÅŸtirme SeÃ§enekleri

### EÄŸer GPU Memory Yetmezse:
```python
BATCH_SIZE = 1
CONTEXT_LENGTH = 384
```

### EÄŸer Daha HÄ±zlÄ± Ä°stersen:
```python
MODEL_SIZE = "tiny"  # 10M params
NUM_EPOCHS = 10
```

### EÄŸer Daha Ä°yi SonuÃ§ Ä°stersen:
```python
MODEL_SIZE = "medium"  # 150M params
NUM_EPOCHS = 20
LEARNING_RATE = 3e-4  # Daha bÃ¼yÃ¼k model = daha kÃ¼Ã§Ã¼k LR
```

---

## ğŸ¯ Sonraki AdÄ±mlar

### EÄŸitim Bittikten Sonra:
1. **Loss grafiÄŸini incele** (`training_loss.png`)
2. **Test et** (`python inference_luna_lm.py`)
3. **FarklÄ± promptlar dene**:
   ```
   "Yapay zekÃ¢"
   "BugÃ¼n hava"
   "Tarih boyunca"
   "Ä°nsan beyni"
   ```
4. **Fine-tuning iÃ§in** ch06/ch07'ye bak

### EÄŸer SonuÃ§lar Ä°yi DeÄŸilse:
- **Overfitting**: EVAL_ITER'i artÄ±r, dropout=0.2 yap
- **Underfitting**: NUM_EPOCHS'u artÄ±r, MODEL_SIZE'Ä± bÃ¼yÃ¼t
- **Memory hatasÄ±**: BATCH_SIZE=1, CONTEXT_LENGTH=384

---

## ğŸ”¥ Kritik Ä°puÃ§larÄ±

1. âœ… **Ä°lk 3 epoch kritik**: Loss hÄ±zla dÃ¼ÅŸmeli
2. âœ… **Val loss train'den 0.2-0.4 yÃ¼ksek olmalÄ±**: Normal
3. âš ï¸ **Val loss artmaya baÅŸlarsa**: Overfitting, dur
4. âœ… **Generated text her epoch daha iyi olmalÄ±**: Kalite gÃ¶stergesi
5. ğŸ”„ **Checkpoint'leri sakla**: En iyi model = en dÃ¼ÅŸÃ¼k val loss

---

## ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Model Size | Params | Context | Batch | Training Time | Expected Loss |
|-----------|--------|---------|-------|---------------|---------------|
| tiny      | 10M    | 512     | 4     | 2-3 saat      | 3.0-3.5       |
| **small** | **50M**| **512** | **2** | **5-10 saat** | **2.5-3.0**   |
| medium    | 150M   | 512     | 1     | 15-20 saat    | 2.0-2.5       |

**Tavsiye**: `small` ile baÅŸla, sonuÃ§lar iyiyse `medium`'a geÃ§!

---

## ğŸ“ Referanslar

Bu parametreler ÅŸu kaynaklara gÃ¶re optimize edildi:
- **LLMs-from-scratch Ch05**: Training best practices
- **Corpus analysis**: 840K token, 112K vocab
- **GPU constraints**: GTX 1650 4GB VRAM
- **Turkish tokenizer**: dbmdz/bert-base-turkish-cased

---

## ğŸ’¡ Son Ã–neriler

1. **SabÄ±rlÄ± ol**: 5-10 saat sÃ¼recek
2. **LoglarÄ± kaydet**: Training Ã§Ä±ktÄ±sÄ±nÄ± bir dosyaya yÃ¶nlendir
   ```bash
   python train_luna_lm.py 2>&1 | tee training.log
   ```
3. **Checkpoint'leri yedekle**: `best_model.pt` Ã§ok deÄŸerli
4. **FarklÄ± seed'ler dene**: Rastgelelik etkisini gÃ¶r
5. **SonuÃ§larÄ± paylaÅŸ**: BaÅŸarÄ±lÄ± olursan community'ye katkÄ±da bulun! ğŸš€

---

**HazÄ±rlandÄ±**: 2025-12-15  
**Corpus**: foundation_corpus.txt (840K tokens)  
**Model**: Luna-LM GPT-small (50M params)  
**Status**: âœ… Ready to train!
