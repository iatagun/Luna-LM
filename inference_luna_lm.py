"""
Luna-LM Model Inference
EÄŸitilmiÅŸ modeli yÃ¼kleyip metin Ã¼retimi yapar
"""

import torch
import json
import os
import glob

from turkish_tokenizer_pretrained import PretrainedTurkishTokenizer
from model import GPTModel, generate_text


# ==================== INFERENCE FONKSÄ°YONLARI ====================

def load_model(checkpoint_dir, checkpoint_name="best_model.pt", device="cpu"):
    """EÄŸitilmiÅŸ modeli yÃ¼kle"""
    
    print(f"Model yÃ¼kleniyor: {checkpoint_dir}")
    
    # Config yÃ¼kle
    config_path = os.path.join(checkpoint_dir, "config.json")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config bulunamadÄ±: {config_path}")
    
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    model_config = config['model_config']
    tokenizer_name = config.get('tokenizer', 'dbmdz/bert-base-turkish-cased')
    
    print(f"  Config yÃ¼klendi:")
    print(f"    Vocab size: {model_config['vocab_size']:,}")
    print(f"    Layers: {model_config['n_layers']}")
    print(f"    Embedding dim: {model_config['emb_dim']}")
    
    # Model oluÅŸtur
    model = GPTModel(model_config)
    
    # Checkpoint yÃ¼kle
    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint bulunamadÄ±: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    print(f"  âœ“ Model yÃ¼klendi: {checkpoint_name}")
    print(f"    Epoch: {checkpoint.get('epoch', 'N/A')}")
    print(f"    Global step: {checkpoint.get('global_step', 'N/A'):,}")
    print(f"    Val loss: {checkpoint.get('val_loss', 'N/A'):.4f}")
    
    # Tokenizer yÃ¼kle
    print(f"\n  Tokenizer yÃ¼kleniyor: {tokenizer_name}")
    tokenizer = PretrainedTurkishTokenizer(tokenizer_name)
    
    return model, tokenizer, model_config


def interactive_mode(model, tokenizer, device, model_config):
    """Ä°nteraktif mod - kullanÄ±cÄ±dan prompt al ve Ã¼ret"""
    
    print("\n" + "="*60)
    print("LUNA-LM Ä°NTERAKTÄ°F MOD")
    print("="*60)
    print("\nKomutlar:")
    print("  - Metin girin ve Enter'a basÄ±n")
    print("  - 'quit' veya 'exit' yazarak Ã§Ä±kÄ±ÅŸ yapÄ±n")
    print("  - 'params' yazarak parametreleri deÄŸiÅŸtirin")
    print("="*60 + "\n")
    
    # VarsayÄ±lan parametreler
    params = {
        'max_tokens': 100,
        'temperature': 0.8,
        'top_k': 50
    }
    
    while True:
        try:
            prompt = input("\nğŸ“ Prompt: ").strip()
            
            if not prompt:
                continue
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("\nGÃ¶rÃ¼ÅŸmek Ã¼zere! ğŸ‘‹")
                break
            
            if prompt.lower() == 'params':
                print("\nMevcut parametreler:")
                print(f"  max_tokens: {params['max_tokens']}")
                print(f"  temperature: {params['temperature']}")
                print(f"  top_k: {params['top_k']}")
                
                try:
                    params['max_tokens'] = int(input("  Yeni max_tokens (Enter=deÄŸiÅŸmez): ") or params['max_tokens'])
                    params['temperature'] = float(input("  Yeni temperature (Enter=deÄŸiÅŸmez): ") or params['temperature'])
                    params['top_k'] = int(input("  Yeni top_k (Enter=deÄŸiÅŸmez): ") or params['top_k'])
                    print("  âœ“ Parametreler gÃ¼ncellendi!")
                except:
                    print("  âœ— GeÃ§ersiz deÄŸer, parametreler deÄŸiÅŸtirilmedi.")
                continue
            
            # Metin Ã¼ret
            print("\nğŸ¤– Luna-LM:")
            generated = generate_text(
                model, tokenizer, device, prompt,
                max_new_tokens=params['max_tokens'],
                temperature=params['temperature'],
                top_k=params['top_k']
            )
            print(generated)
            
        except KeyboardInterrupt:
            print("\n\nGÃ¶rÃ¼ÅŸmek Ã¼zere! ğŸ‘‹")
            break
        except Exception as e:
            print(f"\nâŒ Hata: {e}")


# ==================== MAIN ====================

def main():
    print("\n" + "="*60)
    print("LUNA-LM INFERENCE")
    print("="*60 + "\n")
    
    # Device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}\n")
    
    # Checkpoint dizini bul
    checkpoint_dirs = glob.glob("luna_lm_checkpoints_*")
    
    if not checkpoint_dirs:
        print("âŒ HiÃ§ checkpoint bulunamadÄ±!")
        print("   Ã–nce train_luna_lm.py ile model eÄŸitin.")
        return
    
    # En son checkpoint'i seÃ§
    checkpoint_dir = sorted(checkpoint_dirs)[-1]
    print(f"Checkpoint dizini: {checkpoint_dir}\n")
    
    # Modeli yÃ¼kle
    model, tokenizer, model_config = load_model(checkpoint_dir, device=device)
    
    # Test prompts
    print("\n" + "="*60)
    print("TEST ÃœRETÄ°MLERÄ°")
    print("="*60)
    
    test_prompts = [
        "BugÃ¼n hava Ã§ok gÃ¼zel",
        "Yapay zekÃ¢ teknolojisi",
        "Tarih boyunca insanlÄ±k",
        "Bilim ve teknoloji"
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ“ Prompt: '{prompt}'")
        print("ğŸ¤– Luna-LM:")
        generated = generate_text(
            model, tokenizer, device, prompt,
            max_new_tokens=80,
            temperature=0.8,
            top_k=50
        )
        print(generated)
    
    # Ä°nteraktif mod
    print("\n" + "="*60)
    use_interactive = input("\nÄ°nteraktif moda geÃ§mek ister misiniz? (y/n): ").strip().lower()
    
    if use_interactive == 'y':
        interactive_mode(model, tokenizer, device, model_config)
    else:
        print("\nBitti! Ä°nteraktif mod iÃ§in tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.")


if __name__ == "__main__":
    main()
