{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåô Luna-LM Large ‚Äî H100 Training\n",
    "\n",
    "**Model:** Large (~350M params) ‚Äî emb_dim=1024, n_heads=16, n_layers=24  \n",
    "**GPU:** H100 (80GB VRAM)  \n",
    "**Pipeline:** Pretrain ‚Üí SFT\n",
    "\n",
    "## Adƒ±mlar\n",
    "1. GPU doƒürula\n",
    "2. Google Drive baƒüla\n",
    "3. Repo kur\n",
    "4. Corpus y√ºkle\n",
    "5. Pretrain\n",
    "6. SFT\n",
    "7. Modeli indir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GPU Doƒürula\n",
    "import torch\n",
    "\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "print(f'BF16 destekli: {torch.cuda.is_bf16_supported()}')\n",
    "\n",
    "assert torch.cuda.is_available(), '‚ùå GPU bulunamadƒ±! Runtime > Change runtime type > H100'\n",
    "print('‚úÖ GPU hazƒ±r!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Google Drive Baƒüla\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Checkpoint ve corpus Drive'a kaydedilecek\n",
    "DRIVE_DIR = '/content/drive/MyDrive/LunaLM'\n",
    "import os\n",
    "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
    "print(f'‚úÖ Drive baƒülandƒ±: {DRIVE_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Repo Kur\n",
    "# Se√ßenek A: GitHub'dan clone et (repo public'se)\n",
    "# !git clone https://github.com/iatagun/Luna-LM.git /content/Luna-LM\n",
    "\n",
    "# Se√ßenek B: Drive'daki zip'i kullan\n",
    "# !cp '/content/drive/MyDrive/LunaLM/Luna-LM.zip' /content/\n",
    "# !unzip -q /content/Luna-LM.zip -d /content/\n",
    "\n",
    "# Se√ßenek C: GitHub CLI ile clone (√∂nerilen)\n",
    "!git clone https://github.com/iatagun/Luna-LM.git /content/Luna-LM\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/Luna-LM')\n",
    "os.chdir('/content/Luna-LM')\n",
    "print('‚úÖ Repo hazƒ±r!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Baƒüƒ±mlƒ±lƒ±klarƒ± Y√ºkle\n",
    "!pip install -q transformers datasets\n",
    "\n",
    "# Flash Attention (H100 i√ßin b√ºy√ºk hƒ±z kazanƒ±mƒ±)\n",
    "!pip install -q flash-attn --no-build-isolation\n",
    "\n",
    "print('‚úÖ Paketler y√ºklendi!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Corpus'u Y√ºkle\n",
    "# Se√ßenek A: Drive'dan kopyala\n",
    "CORPUS_DRIVE = '/content/drive/MyDrive/LunaLM/foundation_corpus_clean.txt'\n",
    "CORPUS_LOCAL = '/content/Luna-LM/foundation_corpus_clean.txt'\n",
    "\n",
    "if os.path.exists(CORPUS_DRIVE):\n",
    "    if not os.path.exists(CORPUS_LOCAL):\n",
    "        print('Corpus Drive\\'dan kopyalanƒ±yor...')\n",
    "        !cp '{CORPUS_DRIVE}' '{CORPUS_LOCAL}'\n",
    "    size_gb = os.path.getsize(CORPUS_LOCAL) / 1024**3\n",
    "    print(f'‚úÖ Corpus hazƒ±r: {size_gb:.2f} GB')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Corpus Drive\\'da bulunamadƒ±!')\n",
    "    print('L√ºtfen foundation_corpus_clean.txt dosyasƒ±nƒ± ≈üuraya y√ºkleyin:')\n",
    "    print(f'  {CORPUS_DRIVE}')\n",
    "    print('Veya a≈üaƒüƒ±daki h√ºcreyi √ßalƒ±≈ütƒ±rarak doƒürudan y√ºkleyin:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5b. (Alternatif) Corpus'u doƒürudan y√ºkle\n",
    "# Bu h√ºcreyi sadece corpus Drive'da yoksa √ßalƒ±≈ütƒ±r\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # foundation_corpus_clean.txt se√ß\n",
    "# !mv foundation_corpus_clean.txt /content/Luna-LM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. PRETRAIN ‚Äî Large Model (H100 Optimize)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json, random, time, os\n",
    "from datetime import datetime\n",
    "\n",
    "from luna.tokenizer import PretrainedTurkishTokenizer\n",
    "from luna.data import create_dataloader_pretrained\n",
    "from luna.model import GPTModel, MODEL_CONFIGS\n",
    "from luna.generate import generate_text\n",
    "\n",
    "# ==========================================\n",
    "# AYARLAR ‚Äî H100 (80GB) i√ßin optimize\n",
    "# ==========================================\n",
    "MODEL_SIZE    = 'large'   # emb_dim=1024, n_heads=16, n_layers=24\n",
    "BATCH_SIZE    = 32        # H100: 80GB ‚Üí b√ºy√ºk batch\n",
    "CONTEXT_LEN   = 1024      # Large model i√ßin daha uzun context\n",
    "NUM_EPOCHS    = 3\n",
    "LEARNING_RATE = 6e-4      # Large model i√ßin biraz daha y√ºksek\n",
    "WEIGHT_DECAY  = 0.1\n",
    "EVAL_FREQ     = 500\n",
    "EVAL_ITER     = 20\n",
    "MAX_LINES     = None      # T√ºm corpus (3.6GB)\n",
    "USE_BF16      = True      # H100 BF16 native destekler ‚Äî √ßok daha hƒ±zlƒ±\n",
    "\n",
    "device = torch.device('cuda')\n",
    "dtype  = torch.bfloat16 if USE_BF16 else torch.float32\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'DType:  {dtype}')\n",
    "print(f'Model:  {MODEL_SIZE}')\n",
    "print(f'Batch:  {BATCH_SIZE}')\n",
    "print(f'Ctx:    {CONTEXT_LEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = PretrainedTurkishTokenizer('dbmdz/bert-base-turkish-cased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f'Vocab: {vocab_size:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus Y√ºkle\n",
    "corpus_path = '/content/Luna-LM/foundation_corpus_clean.txt'\n",
    "\n",
    "print('Corpus y√ºkleniyor...')\n",
    "lines = []\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if MAX_LINES and i >= MAX_LINES:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "        if (i+1) % 500000 == 0:\n",
    "            print(f'  {i+1:,} satƒ±r okundu...')\n",
    "\n",
    "print(f'‚úÖ {len(lines):,} satƒ±r y√ºklendi')\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(lines)\n",
    "\n",
    "split = int(0.95 * len(lines))   # 95/5 split ‚Äî b√ºy√ºk corpus\n",
    "train_text = '\\n'.join(lines[:split])\n",
    "val_text   = '\\n'.join(lines[split:])\n",
    "del lines\n",
    "\n",
    "print(f'Train: {len(train_text)/1e9:.2f}B karakter')\n",
    "print(f'Val:   {len(val_text)/1e6:.1f}M karakter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = create_dataloader_pretrained(\n",
    "    train_text, tokenizer,\n",
    "    batch_size=BATCH_SIZE, max_length=CONTEXT_LEN, stride=CONTEXT_LEN, shuffle=True\n",
    ")\n",
    "val_loader = create_dataloader_pretrained(\n",
    "    val_text, tokenizer,\n",
    "    batch_size=BATCH_SIZE, max_length=CONTEXT_LEN, stride=CONTEXT_LEN, shuffle=False\n",
    ")\n",
    "\n",
    "print(f'Train batches: {len(train_loader):,}')\n",
    "print(f'Val batches:   {len(val_loader):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Olu≈ütur\n",
    "cfg = MODEL_CONFIGS[MODEL_SIZE]\n",
    "model_config = {\n",
    "    'vocab_size':      vocab_size,\n",
    "    'context_length':  CONTEXT_LEN,\n",
    "    'emb_dim':         cfg['emb_dim'],\n",
    "    'n_heads':         cfg['n_heads'],\n",
    "    'n_layers':        cfg['n_layers'],\n",
    "    'drop_rate':       0.1,\n",
    "    'qkv_bias':        False,\n",
    "}\n",
    "\n",
    "model = GPTModel(model_config)\n",
    "\n",
    "# BF16 i√ßin cast\n",
    "if USE_BF16:\n",
    "    model = model.to(dtype=torch.bfloat16)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# torch.compile ‚Äî H100'de ~%30 hƒ±z artƒ±≈üƒ±\n",
    "model = torch.compile(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'‚úÖ Model hazƒ±r: {total_params/1e6:.1f}M parametre')\n",
    "print(f'   emb_dim={cfg[\"emb_dim\"]}, n_heads={cfg[\"n_heads\"]}, n_layers={cfg[\"n_layers\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer & Scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.95),   # GPT-3 style\n",
    "    eps=1e-8,\n",
    "    fused=True           # H100 i√ßin fused optimizer\n",
    ")\n",
    "\n",
    "total_steps   = len(train_loader) * NUM_EPOCHS\n",
    "warmup_steps  = int(total_steps * 0.02)   # %2 warmup ‚Äî b√ºy√ºk model\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return step / max(warmup_steps, 1)\n",
    "    progress = (step - warmup_steps) / max(total_steps - warmup_steps, 1)\n",
    "    import math\n",
    "    return 0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress))  # min_lr = 10% of max\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(f'Total steps:  {total_steps:,}')\n",
    "print(f'Warmup steps: {warmup_steps:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dir (Drive'a kaydet ‚Äî session bitse bile korunur)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "save_dir  = f'{DRIVE_DIR}/pretrain_large_{timestamp}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "with open(f'{save_dir}/config.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'model_config': model_config,\n",
    "        'training_config': {\n",
    "            'model_size': MODEL_SIZE,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_epochs': NUM_EPOCHS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'context_length': CONTEXT_LEN,\n",
    "            'use_bf16': USE_BF16,\n",
    "        },\n",
    "        'tokenizer': 'dbmdz/bert-base-turkish-cased',\n",
    "        'timestamp': timestamp,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f'‚úÖ Save dir: {save_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETRAIN LOOP\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "train_losses, val_losses, tokens_seen_log = [], [], []\n",
    "tokens_seen  = 0\n",
    "global_step  = 0\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print('='*60)\n",
    "print('PRETRAIN BA≈ûLIYOR')\n",
    "print('='*60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    ep_start = time.time()\n",
    "    \n",
    "    for input_batch, target_batch in train_loader:\n",
    "        input_batch  = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(dtype=torch.bfloat16, enabled=USE_BF16):\n",
    "            logits = model(input_batch)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.flatten(0, 1), target_batch.flatten()\n",
    "            )\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        tokens_seen += input_batch.numel()\n",
    "        global_step += 1\n",
    "        \n",
    "        if global_step % EVAL_FREQ == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # Train loss sample\n",
    "                tl = 0\n",
    "                for k, (ib, tb) in enumerate(train_loader):\n",
    "                    if k >= EVAL_ITER: break\n",
    "                    with autocast(dtype=torch.bfloat16, enabled=USE_BF16):\n",
    "                        lg = model(ib.to(device))\n",
    "                        tl += torch.nn.functional.cross_entropy(lg.flatten(0,1), tb.to(device).flatten()).item()\n",
    "                tl /= EVAL_ITER\n",
    "                \n",
    "                # Val loss sample\n",
    "                vl = 0\n",
    "                for k, (ib, tb) in enumerate(val_loader):\n",
    "                    if k >= EVAL_ITER: break\n",
    "                    with autocast(dtype=torch.bfloat16, enabled=USE_BF16):\n",
    "                        lg = model(ib.to(device))\n",
    "                        vl += torch.nn.functional.cross_entropy(lg.flatten(0,1), tb.to(device).flatten()).item()\n",
    "                vl /= EVAL_ITER\n",
    "            \n",
    "            train_losses.append(tl)\n",
    "            val_losses.append(vl)\n",
    "            tokens_seen_log.append(tokens_seen)\n",
    "            \n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Ep {epoch+1} | Step {global_step:,} | '\n",
    "                  f'Train: {tl:.4f} | Val: {vl:.4f} | '\n",
    "                  f'LR: {lr:.2e} | Tokens: {tokens_seen/1e6:.1f}M')\n",
    "            \n",
    "            if vl < best_val_loss:\n",
    "                best_val_loss = vl\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'epoch': epoch, 'step': global_step,\n",
    "                    'val_loss': vl, 'tokens_seen': tokens_seen,\n",
    "                }, f'{save_dir}/best_model.pt')\n",
    "                print(f'  ‚úÖ Best model! Val: {vl:.4f}')\n",
    "            \n",
    "            model.train()\n",
    "    \n",
    "    ep_time = (time.time() - ep_start) / 60\n",
    "    print(f'\\nEpoch {epoch+1} tamamlandƒ± ({ep_time:.1f} dk)\\n')\n",
    "    \n",
    "    # Epoch checkpoint\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch},\n",
    "               f'{save_dir}/epoch_{epoch+1}.pt')\n",
    "\n",
    "print('‚úÖ PRETRAIN TAMAMLANDI!')\n",
    "print(f'Best val loss: {best_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Grafiƒüi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(tokens_seen_log, train_losses, label='Train')\n",
    "ax.plot(tokens_seen_log, val_losses,   label='Val', linestyle='--')\n",
    "ax.set_xlabel('Tokens Seen')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Luna-LM Large ‚Äî Pretrain')\n",
    "ax.legend(); ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{save_dir}/pretrain_loss.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SFT ‚Äî Alpaca Turkish (82K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Dataseti ƒ∞ndir\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "SFT_JSONL = '/content/Luna-LM/sft/sft_dataset.jsonl'\n",
    "\n",
    "if not os.path.exists(SFT_JSONL):\n",
    "    print('SFT dataset indiriliyor...')\n",
    "    ds = load_dataset('cenfis/alpaca-turkish-combined')\n",
    "    with open(SFT_JSONL, 'w', encoding='utf-8') as f:\n",
    "        for row in ds['train']:\n",
    "            user = row['instruction']\n",
    "            if row.get('input'):\n",
    "                user += f\"\\n{row['input']}\"\n",
    "            if user and row['output']:\n",
    "                f.write(json.dumps({'user': user, 'assistant': row['output']}, ensure_ascii=False) + '\\n')\n",
    "    print(f'‚úÖ SFT dataset hazƒ±r: {SFT_JSONL}')\n",
    "else:\n",
    "    print(f'‚úÖ SFT dataset mevcut: {SFT_JSONL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT Eƒüitimi\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['python', 'sft/train_sft.py'],\n",
    "    capture_output=False,\n",
    "    text=True,\n",
    "    cwd='/content/Luna-LM'\n",
    ")\n",
    "\n",
    "# NOT: train_sft.py'deki BATCH_SIZE'ƒ± H100 i√ßin 32 yap\n",
    "# ve en son pretrain checkpoint'ini doƒüru g√∂ster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli Drive'a Kopyala (session bitince korunur)\n",
    "!cp -r /content/Luna-LM/checkpoints/sft_* '{DRIVE_DIR}/'\n",
    "print('‚úÖ SFT model Drive\\'a kopyalandƒ±!')\n",
    "\n",
    "# zip'le ve indir (isteƒüe baƒülƒ±)\n",
    "# !zip -r /content/luna_large_sft.zip '{save_dir}/best_model.pt'\n",
    "# from google.colab import files\n",
    "# files.download('/content/luna_large_sft.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hƒ±zlƒ± Test\n",
    "from luna.utils import load_model\n",
    "from luna.generate import generate_text\n",
    "\n",
    "SYSTEM_PROMPT = ('Senin adƒ±n Luna. Amacƒ±n insanlara yardƒ±mcƒ± olmak ve sorulara a√ßƒ±k, '\n",
    "                 'anla≈üƒ±lƒ±r cevaplar vermektir. Emin olmadƒ±ƒüƒ±n konularda bunu belirtir, '\n",
    "                 'uydurma bilgi eklemezsin.')\n",
    "\n",
    "def chat(model, tokenizer, device, question, **kwargs):\n",
    "    prompt = f'<system>{SYSTEM_PROMPT}</system>\\n<user>{question}</user>\\n<assistant>'\n",
    "    out = generate_text(model, tokenizer, device, prompt, max_new_tokens=200,\n",
    "                        temperature=0.7, top_k=50, repetition_penalty=1.2, **kwargs)\n",
    "    if '<assistant>' in out:\n",
    "        ans = out.split('<assistant>')[-1]\n",
    "        for s in ['</assistant>', '<user>', '<system>', '[SEP]']:\n",
    "            ans = ans.split(s)[0]\n",
    "        return ans.strip()\n",
    "    return out\n",
    "\n",
    "# En son SFT checkpoint'i y√ºkle\n",
    "import glob\n",
    "sft_dirs = sorted(glob.glob('/content/Luna-LM/checkpoints/sft_*'))\n",
    "model_t, tokenizer_t, _ = load_model(sft_dirs[-1], device)\n",
    "\n",
    "test_questions = [\n",
    "    'G√ºne≈ü hangi y√∂nden doƒüar?',\n",
    "    'Yapay zeka nedir?',\n",
    "    'T√ºrkiye\\'nin ba≈ükenti neresidir?',\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f'‚ùì {q}')\n",
    "    print(f'ü§ñ {chat(model_t, tokenizer_t, device, q)}\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "H100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
