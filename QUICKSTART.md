# Luna-LM: TÃ¼rkÃ§e Foundation Language Model

SÄ±fÄ±rdan TÃ¼rkÃ§e dil modeli eÄŸitimi - PyTorch implementasyonu.

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

```bash
# 1. Gereksinimleri yÃ¼kle
pip install torch transformers tokenizers matplotlib

# 2. Model eÄŸitimini baÅŸlat
python train_luna_lm.py

# 3. EÄŸitilmiÅŸ modeli test et
python inference_luna_lm.py
```

**Bu kadar! âœ…** Ä°lk eÄŸitim ~2-4 saat sÃ¼rer.

---

## ğŸ“ Ana Dosyalar

| Dosya | AÃ§Ä±klama |
|-------|----------|
| **[train_luna_lm.py](train_luna_lm.py)** | â­ Model eÄŸitimi (buradan baÅŸlayÄ±n!) |
| **[inference_luna_lm.py](inference_luna_lm.py)** | Model test ve kullanÄ±m |
| **[foundation_corpus.txt](foundation_corpus.txt)** | TÃ¼rkÃ§e eÄŸitim verisi (25K+ satÄ±r) |
| **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)** | ğŸ“– DetaylÄ± eÄŸitim rehberi |
| **[TURKISH_TOKENIZER_README.md](TURKISH_TOKENIZER_README.md)** | Tokenizer kullanÄ±m rehberi |

---

## ğŸ¯ Model BoyutlarÄ±

| Boyut | Parametreler | RAM | SÃ¼re | Durum |
|-------|--------------|-----|------|-------|
| **tiny** | 10M | 2-4 GB | 30-60 dk | HÄ±zlÄ± test |
| **small** | 50M | 4-8 GB | 2-4 saat | âœ… Ã–nerilen |
| **medium** | 150M | 8-16 GB | 6-12 saat | Ä°yi sonuÃ§ |

`train_luna_lm.py` iÃ§inde `MODEL_SIZE` deÄŸiÅŸtirin.

---

## ğŸ’¡ EÄŸitim SÃ¼reci

### 1. Veriyi HazÄ±rla
- `foundation_corpus.txt` (25,832 satÄ±r TÃ¼rkÃ§e metin)
- Otomatik train/val split (%90/%10)

### 2. Tokenizer
- Pretrained TÃ¼rkÃ§e BERT tokenizer (32K vocab)
- Alternatif: Custom BPE tokenizer eÄŸitebilirsiniz

### 3. Model Mimarisi
- GPT-benzeri transformer decoder
- Multi-head attention
- Autoregressive language modeling

### 4. EÄŸitim
```
Epoch 1/10 | Step 100 | Train Loss: 8.24 | Val Loss: 8.12
  âœ“ En iyi model kaydedildi!
  
  ğŸ“ Ã–rnek: "BugÃ¼n hava Ã§ok gÃ¼zel ve insanlar..."

Epoch 5/10 | Step 2800 | Train Loss: 4.15 | Val Loss: 4.28
...
```

### 5. Ã‡Ä±ktÄ±lar
```
luna_lm_checkpoints_20251214_153045/
â”œâ”€â”€ best_model.pt           # En iyi model
â”œâ”€â”€ epoch_1.pt, ...         # Her epoch checkpoint
â”œâ”€â”€ config.json             # Model config
â””â”€â”€ training_loss.png       # Loss grafiÄŸi
```

---

## ğŸ® Model KullanÄ±mÄ±

### Komut SatÄ±rÄ±
```bash
python inference_luna_lm.py
```

### Python Kodu
```python
from inference_luna_lm import load_model, generate_text
import torch

# Model yÃ¼kle
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model, tokenizer, config = load_model(
    "luna_lm_checkpoints_XXXXXXXX", 
    device=device
)

# Metin Ã¼ret
text = generate_text(
    model, tokenizer, device,
    prompt="Yapay zekÃ¢",
    max_new_tokens=100,
    temperature=0.8
)
print(text)
```

### Ä°nteraktif Mod
```
ğŸ“ Prompt: Tarih boyunca insanlÄ±k
ğŸ¤– Luna-LM:
Tarih boyunca insanlÄ±k birÃ§ok zorlukla karÅŸÄ±laÅŸtÄ±. 
Medeniyetler yÃ¼kseldi, Ã§Ã¶ktÃ¼ ve yeniden doÄŸdu...

ğŸ“ Prompt: quit
```

---

## ğŸ”§ Ã–zelleÅŸtirme

### Model Boyutunu DeÄŸiÅŸtir
```python
# train_luna_lm.py iÃ§inde
MODEL_SIZE = "medium"  # tiny, small, medium
```

### Hyperparameter Ayarla
```python
BATCH_SIZE = 4           # GPU memory'e gÃ¶re
CONTEXT_LENGTH = 256     # Max sequence length
NUM_EPOCHS = 10          # EÄŸitim epoch sayÄ±sÄ±
LEARNING_RATE = 3e-4     # Ã–ÄŸrenme hÄ±zÄ±
```

### Custom Tokenizer Kullan
```python
# Kendi tokenizer'Ä±nÄ±zÄ± eÄŸitin
python turkish_tokenizer_huggingface.py

# train_luna_lm.py iÃ§inde deÄŸiÅŸtir
from turkish_tokenizer_huggingface import HFTokenizerWrapper
tokenizer = HFTokenizerWrapper('turkish_hf_tokenizer.json')
```

---

## ğŸ“Š Beklenen SonuÃ§lar

### Loss DeÄŸerleri (10 epoch sonrasÄ±)
- **Train Loss**: 3.0-3.5
- **Val Loss**: 3.2-3.8

### Metin Kalitesi
- âœ… TÃ¼rkÃ§e kelimeler ve cÃ¼mleler Ã¼retir
- âœ… Temel gramer kurallarÄ±nÄ± takip eder
- âœ… Konuya uygun kelime seÃ§er
- âš ï¸ Uzun paragraflar iÃ§in daha fazla eÄŸitim gerekir

---

## ğŸ› Sorun Giderme

### GPU Memory HatasÄ±
```python
BATCH_SIZE = 2          # veya 1
MODEL_SIZE = "tiny"     # kÃ¼Ã§Ã¼k model
CONTEXT_LENGTH = 128    # kÄ±sa context
```

### YavaÅŸ EÄŸitim
- GPU kullandÄ±ÄŸÄ±nÄ±zdan emin olun: `torch.cuda.is_available()`
- CUDA yÃ¼klÃ¼ mÃ¼ kontrol edin
- KÃ¼Ã§Ã¼k model ile test edin

### Loss DÃ¼ÅŸmÃ¼yor
```python
LEARNING_RATE = 5e-4    # artÄ±r
NUM_EPOCHS = 20         # daha fazla epoch
```

### Ãœretilen Metinler KÃ¶tÃ¼
- Daha fazla epoch eÄŸitin (loss < 3.0 hedefleyin)
- Temperature ayarlayÄ±n (0.7-1.0)
- Daha fazla veri ekleyin

---

## ğŸ“š DetaylÄ± DokÃ¼mantasyon

- **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)**: TÃ¼m eÄŸitim detaylarÄ±, optimizasyon, sorun giderme
- **[TURKISH_TOKENIZER_README.md](TURKISH_TOKENIZER_README.md)**: Tokenizer seÃ§enekleri ve kullanÄ±mÄ±
- **[LLMs-from-scratch/](LLMs-from-scratch/)**: Orijinal kod ve eÄŸitim materyalleri

---

## ğŸ“ Sonraki AdÄ±mlar

### 1. Fine-tuning
Ã–zel gÃ¶revler iÃ§in model ince ayarÄ±:
- Text Classification ([ch06](LLMs-from-scratch/ch06/))
- Instruction Following ([ch07](LLMs-from-scratch/ch07/))

### 2. Veri ArtÄ±rma
Daha fazla TÃ¼rkÃ§e metin ekleyin:
- Wikipedia
- Haberler
- Kitaplar
- Akademik metinler

### 3. Model BÃ¼yÃ¼tme
```python
MODEL_SIZE = "medium"    # 150M parametre
CONTEXT_LENGTH = 512     # Daha uzun context
NUM_EPOCHS = 20          # Daha fazla epoch
```

### 4. Deployment
- ONNX export
- Quantization (INT8)
- FastAPI ile API servisi
- Streamlit UI

---

## ğŸ“ˆ Ã–rnek Ã‡Ä±ktÄ±lar

### Epoch 1 (BaÅŸlangÄ±Ã§)
```
Prompt: "BugÃ¼n hava"
Output: "Ã§ok ve en ile bir iÃ§in..."
```

### Epoch 5 (GeliÅŸme)
```
Prompt: "BugÃ¼n hava"
Output: "Ã§ok gÃ¼zel ve insanlar dÄ±ÅŸarÄ±da yÃ¼rÃ¼yÃ¼ÅŸ yapÄ±yor."
```

### Epoch 10 (Ä°yi SonuÃ§)
```
Prompt: "BugÃ¼n hava"
Output: "Ã§ok gÃ¼zel. GÃ¶kyÃ¼zÃ¼ aÃ§Ä±k ve gÃ¼neÅŸ parlÄ±yor. 
Ä°nsanlar parklarda yÃ¼rÃ¼yÃ¼ÅŸ yapÄ±yor ve Ã§ocuklar 
oyun oynuyor."
```

---

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yapÄ±n
2. Feature branch oluÅŸturun
3. Commit yapÄ±n
4. Push edin
5. Pull Request aÃ§Ä±n

---

## ğŸ“ Lisans

Bu proje MIT lisansÄ± altÄ±ndadÄ±r. LLMs-from-scratch kodu Apache 2.0 lisansÄ± kullanÄ±r.

---

## ğŸ™ TeÅŸekkÃ¼rler

- **Sebastian Raschka**: [LLMs from Scratch](https://github.com/rasbt/LLMs-from-scratch) kitabÄ± ve kodu
- **DBMDz**: TÃ¼rkÃ§e BERT tokenizer
- **Hugging Face**: Transformers ve Tokenizers kÃ¼tÃ¼phaneleri

---

## ğŸ“ Ä°letiÅŸim

- **GitHub Issues**: Sorular ve bug raporlarÄ±
- **Discussions**: Genel tartÄ±ÅŸmalar ve yardÄ±m

---

## â­ HÄ±zlÄ± Komutlar

```bash
# EÄŸitim baÅŸlat
python train_luna_lm.py

# Model test et
python inference_luna_lm.py

# Custom tokenizer eÄŸit
python turkish_tokenizer_huggingface.py

# Tokenizer test et
python turkish_tokenizer_pretrained.py

# GPU kontrolÃ¼
python -c "import torch; print(torch.cuda.is_available())"
```

---

**BaÅŸarÄ±lar! ğŸš€** Luna-LM'inizi eÄŸitin ve kendi TÃ¼rkÃ§e dil modelinizi oluÅŸturun!
